[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Propensity Score Analysis for Medical Research: A Primer and Tutorial",
    "section": "",
    "text": "Introduction\nPropensity score analysis is a popular method of adjusting for confounding in observational studies, i.e., studies where patients are not randomly assigned into treatment groups. Despite its popularity in medical research, there are many nuances to the method that are often missed by researchers, including about the assumptions required, the quantities that can be estimated, and the correct procedures for performing and reporting an analysis. The goal of this guide is to summarize best practices in propensity score analysis for medical researchers, highlighting the decisions researchers must make to validly perform and interpret an analysis. This guide is not a substitute for a PhD in biostatistics or even a course in propensity score analysis; it should be seen as a starting point that synthesizes the existing literature and provides references for further reading to deepen one’s understanding of the methods involved.\nIn the same way a statistician cannot learn surgery well enough to perform it by reading a tutorial, nor should physicians be expected to perform a complex statistical analysis by reading one. However, a patient undergoing surgery would greatly benefit from understanding the risks, procedures, and possible outcomes of a surgery, even if they are not performing it themselves; similarly, a physician can be better prepared to work with a trained statistician after understanding the assumptions, decision points, and limitations of a statistical method.\nAlthough propensity score analysis is an analytic method, the key to performing it well is to understand the theoretical and substantive considerations relevant to the method; therefore, this guide will focus on those aspects. For the more applied parts of the tutorial, R code and output will be presented to demonstrate the procedure.\nThis document will describe the three basic steps of performing a propensity score analysis: 1) planning the analysis, 2) running the analysis, and 3) reporting the analysis. In Chapter 1, we will describe the conceptual steps that must be done to determine what options will be selected in subsequent steps and to align the analysis with the quantity of interest. In Chapter 2 and the chapters that follow, we provide step-by-step instructions for implementing the computational steps of performing the analysis, including fitting models and estimating quantities of interest. In Chapter 10, we provide guidelines for reporting the results of an analysis, including information to include in tables and figures and information necessary for another researchers to replicate your findings. Finally, in Chapter 11 and the chapters that follow, we present an example analysis that includes R code to implement the methods."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Propensity Score Analysis for Medical Research: A Primer and Tutorial",
    "section": "",
    "text": "For the sake of this tutorial, we will classify all methods that manipulate the sample using observed variables to remove confounding to fall under the scope of “propensity score analysis”, even though propensity scores are not always used to do this. Many of the best performing methods for this task do not use propensity scores at all.↩︎"
  },
  {
    "objectID": "planning.html#types-of-questions-that-can-be-answered",
    "href": "planning.html#types-of-questions-that-can-be-answered",
    "title": "1  Planning the Analysis",
    "section": "1.1 Types of questions that can be answered",
    "text": "1.1 Types of questions that can be answered\nThere are several ways to ask causal questions: one is to ask what are the causes of observed variation in outcomes (e.g., why certain patients experience remission and others don’t), and another is to ask what the causal effect of one variable is on an outcome (e.g., whether taking aspirin reduces heart attack risk). Propensity score analysis is only appropriate for answering the second type of question, i.e., “What is the effect of a treatment on an outcome?” It is not useful for discovering predictors of an outcome, for developing clinical prediction models, for identifying which variables are important drivers of the outcome, etc. To perform propensity score analysis, one must have a single well-defined treatment.\nThe type of quantity propensity score analysis is best suited to answer is the total effect of the treatment on the outcome. The total effect refers to an effect that ignores any intermediate pathways. For example, a drug might affect survival by stimulating the immune response in a patient; propensity score analysis can help answer the question of whether the treatment affects the immune response or whether the treatment affects survival, but it cannot answer the mechanistic question of whether the effect of treatment on the survival is due to its effect on the immune response. That type of question is a “mediation” question, and though propensity score analysis can play a role in such an analysis, in its basic and most common form, it is not able to answer such questions.\nPropensity score analysis is sometimes used in disparities research, i.e., to identify whether a disparity exists on a single patient dimension (e.g., sex or race) after controlling for differences in relevant variables between these groups. For example, one might notice that pain management prescription amounts differ between male and female patients with a given diagnosis; propensity score analysis can be used to create groups of male and female patients that look similar on variables that might be relevant to explaining away that disparity, such as age, comorbidities, weight, height, etc., so that the only explanation left for the disparity is a personal bias by prescribing physicians.\nThis type of analysis has several problems and will not be discussed further here (see Huber (2015) for a discussion of these problems). However, it does identify that propensity score analysis can be used to create comparable groups no matter what those groups are to be used for; the analysis itself is agnostic to how those groups will be compared and whether that comparison represents a valid causal effect or disparity. It is the assumptions behind the analysis that allow for causal inference; these assumptions are described later in this section."
  },
  {
    "objectID": "planning.html#choosing-an-estimand",
    "href": "planning.html#choosing-an-estimand",
    "title": "1  Planning the Analysis",
    "section": "1.2 Choosing an estimand",
    "text": "1.2 Choosing an estimand\nAn estimand is simply the quantity being estimated, i.e., the quantity of interest from a study. Although one might simply say that the estimand is the treatment effect, there are nuances to an estimand that are critical to articulate to be able to validly interpret the resulting estimate and to be able to make choices in the analysis that target the estimand. The concept of an estimand is also present in randomized trials, and many of the considerations for these estimands also apply to those in observational studies, though there are some additional considerations. Kahan et al. (2023) and Han and Zhou (2023) provide nice overviews of the components of an estimand in clinical trials.\nSome components of an estimand common to observational studies and randomized trials include who the effect is estimated for in the presence of noncompliance (e.g., just those who actually received the treatment or all those who were assigned treatment), how the effect is measured (e.g., as a risk ratio, hazard ratio, or odds ratio), the time scale of the outcome (e.g., death at 12 months or 24 months), and how intercurrent events (e.g., death before a non-death clinical endpoint) are incorporated. Two components of an estimand that are particularly important to consider in propensity score analysis are whether the effect is to be marginal or conditional and for which subset of the study population the effect is meant to generalize; we focus on these in this tutorial, but that is not a reason to ignore the others.\n\n1.2.1 Marginal and conditional effects\nA marginal effect is a comparison between the expected potential outcome under treatment and the expected potential outcome under control. This is the same quantity estimated in randomized trials without blocking or covariate adjustment and is particularly useful for quantifying the overall effect of a policy or population-wide intervention. A conditional effect is the comparison between the expected potential outcomes in the treatment groups within strata. This is useful for identifying the effect of a treatment for an individual patient or a subset of the population.\nAlthough conditional effects are often more useful for clinical decision-making, they require far stricter assumptions about the relationship between confounders and the outcome and are not well suited for propensity score analysis. Estimating conditional effects either involves performing the analysis within subgroups of the sample (which can dramatically shrink the available data and yield imprecise estimates) or using an outcome model that presupposes a very specific and unrealistic functional form (e.g., using the coefficient on treatment in a logistic regression model with confounders included). We will focus instead on marginal effects, which can be estimated using the full dataset and don’t require such assumptions.\n\n\n1.2.2 The target population\nThe target population refers to the population to which the effect is meant to generalize. Selecting this population is critical in propensity score analysis because it determines how specific steps of the analysis proceed and how to interpret them. There are four common estimands that can be targeted in propensity score analysis:\n\nThe average treatment effect in the population (ATE) - the average difference in outcomes between a scenario in which all units were treated and a scenario in which no units were treated. This is useful for determining universal policies or broadly understanding the effect of a treatment (e.g., Does this treatment work on average?).\nThe average treatment effect in the treated (ATT) - the average difference between the observed outcomes for the treated units and the outcomes the treated units would have had had they not been treated. It can be interpreted as the effect of withholding treatment from those who would otherwise receive it. This is useful for estimating the effects of potentially dangerous exposures or experimental procedures that would be given to patients like those currently receiving it.\nThe average treatment effect in the control (ATC) - the average difference between the observed outcomes for the untreated units and the outcomes the untreated units would have had had they been treated. It can be interpreted as the effect of expanding treatment to those who would not otherwise receive it.\nThe average treatment effect in the overlap (ATO) - the average difference in outcomes for those at clinical equipoise (i.e., the “overlap” population) were they to receive treatment and were they not to receive treatment. Although the scope of the ATO is limited and sometimes vague (i.e., because there are many ways to statistically define the overlap population), these estimates tend to be the least biased, most precise, and most resistant to biases due to unobserved variables like patient frailty.\n\nThese estimands, a guide of how to choose among them, and the specific techniques that can be used to estimate them are described in detail in Greifer and Stuart (2021)."
  },
  {
    "objectID": "planning.html#meeting-assumptions",
    "href": "planning.html#meeting-assumptions",
    "title": "1  Planning the Analysis",
    "section": "1.3 Meeting assumptions",
    "text": "1.3 Meeting assumptions\nThere are a number of assumptions that are critical to being able to interpret the effects estimated using propensity score analysis as causal. Other methods for estimating causal effects may involve different assumptions; the assumptions listed here apply only to propensity score analysis and other methods of adjustment that rely on adjusting for observed variables, such as regression adjustment (Matthay et al. 2020). If these assumptions are violated, the link between the statistical quantity estimated by propensity score analysis and the causal quantity desired by the researcher is broken.\nPropensity score analysis can only be used to estimate causal effects when these assumptions are met. In this sense, propensity score analysis is not a “causal inference method”, it is a method of estimating a statistical quantity (the adjusted association between the treatment and outcome), which can only be interpreted as a causal effect when these assumptions are met. The key assumptions are satisfaction of the backdoor criterion, positivity, and consistency. These are in addition to other assumptions that underlie the methods involved, such as assumptions about missing data if any are present and assumptions about correct measurement of the confounders, treatment, and outcome.\n\n1.3.1 The backdoor criterion\nThe backdoor criterion is that there are no “backdoor paths” from the treatment to the outcome. A backdoor path is a causal chain from the treatment to the outcome through a common cause of the treatment and outcome. Satisfaction of the backdoor criterion means that, conditional on the set of variables to be adjusted for, there are no backdoor paths from the treatment to the outcome, and the only association between the treatment and the outcome is due to the causal effect of the treatment on the outcome. In addition, no variables that induce bias have been adjusted for; these include variables caused by the treatment or the outcome. VanderWeele (2019) provides a clear guide on how to meet this assumption.\nSatisfaction of the backdoor criterion is also known as the assumption of “strong ignorability” (Paul R. Rosenbaum and Rubin 1983), “conditional exchangeability” (Hernán and Robins 2006), “selection on observables”, or, simply, no unmeasured confounding. Meeting this assumption requires a researcher to have collected a sufficient set of variables that closes all backdoor paths without opening any biasing paths. This assumption is often considered hard to meet when treatment has not been randomly assigned or the assignment mechanism is unknown, which is why methods that rely on this assumption when used in observational studies are often viewed with suspicion and why claiming causality from observational studies is dangerous.\nThere are a few strategies for meeting this assumption. One is to include all measured variables in the analysis, hoping that the set of measured variables is sufficient to satisfy the criterion. This is known as the “kitchen sink” approach. When the temporal ordering of the variables is clear (i.e., it can be assured that all of the variables to be adjusted for precede treatment), this can be an effective strategy, especially if many variables jointly act as proxies for possibly unmeasured confounders (Brookhart et al. 2010). It is critical that all variables adjusted for are not even possible affected by the treatment or outcome. Another, more principled strategy is to draw a causal diagram, known as a DAG, that represents what is known about the causal system under study and can be used to select the specific variables that are and are not necessary to close all backdoor paths (Greenland, Pearl, and Robins 1999). Either way, researchers must be prepared to justify why they adjusted for the variables they did and why adjustment for these variables is sufficient to satisfy the backdoor criterion.\n\n\n1.3.2 Positivity\nPositivity is the assumption that all units are eligible to be either treated or untreated (Westreich and Cole 2010). The idea is that if some units are ineligible to be treated, it doesn’t make sense to try to infer what would have happened to them had they been treated.\nPositivity is an assumption about treatment assignment, but there is an empirical version of it often known as “overlap” (Fogarty et al. 2016). Overlap is the extent to which the distributions of covariates in the treated and untreated groups overlap with each other. Even if positivity holds (i.e., all patients in the study population are theoretically eligible for either treatment), it may be that in the sample, there are patient profiles that are absent from one group (Westreich and Cole 2010). In the absence of good overlap, it will be challenging or impossible to use propensity score analysis to make the treatment groups resemble each other on the measured confounders; one is forced either to extrapolate inferences about the groups or to change the target population to one with some overlap (e.g., by targeting the ATO) (King and Zeng 2006).\n\n\n1.3.3 Consistency\nConsistency is the assumption that there are no unmeasured versions of treatment, i.e., that the treatment values are well-defined (Hernán and Taubman 2008). Consistency might be violated if there are multiple doses the treatment could take but it is only measured as its presence or absence (Cole and Frangakis 2009). A component of consistency is the stable unit treatment value assumption (SUTVA), which requires that the treatment statuses of other patients do not affect the outcomes of a given patient (Paul R. Rosenbaum 2007). This would also constitute a different “version” of treatment; for example, being given a vaccine when no other patients are vaccinated is a different version of the treatment from being given a vaccine when all other patients are vaccinated (assuming the patients can interact with each other) (Cole and Frangakis 2009).\nWhen treatment versions are identifiable (e.g., a measured dose), there are extensions for propensity score analysis that can be used for multi-category or continuous treatments (Imai and Van Dyk 2004). Methods have also been developed for estimating causal effects in the presence of interference, a SUTVA violation (Tchetgen and VanderWeele 2012)."
  },
  {
    "objectID": "planning.html#summary",
    "href": "planning.html#summary",
    "title": "1  Planning the Analysis",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nPropensity score analysis is not a general-purpose causal inference method; it is a statistical method that can be used to answer a specific type of question, i.e., the total effect of a treatment on an outcome. The research question must be articulated clearly with an estimand specified prior to the analysis. The estimand must consist not only of the components that are common to randomized trials (e.g., the effect measure, the time scale of the outcome, etc.) but also of the components that are more specific to the analysis of observational studies, which include whether the effect is marginal or conditional and to which target population the effect is meant to generalize. Theses choice are described clearly by Kahan et al. (2023) and Greifer and Stuart (2021), which are highly recommended.\nPropensity score analysis requires certain assumptions for the effect estimate to be validly interpreted as causal, which include satisfaction of the backdoor criterion, positivity, and consistency (Hernán and Robins 2006). These assumptions must be assessed by appealing to substantive knowledge about the causal system under study and the variables that are available to the researcher. Satisfaction of the backdoor criterion requires that there is no unmeasured confounding and no variables caused by the treatment or outcome are adjusted for. Positivity requires that all units are eligible to receive either treatment. Consistency requires that there are no unmeasured versions of treatment, including versions defined by the treatment status of other patients in the study. These assumptions and the choices required to satisfy and assess them are described in VanderWeele (2019), Westreich and Cole (2010), and Hernán and Taubman (2008), which are highly recommended.\n\n\n\n\nBrookhart, M. Alan, Til Stürmer, Robert J. Glynn, Jeremy Rassen, and Sebastian Schneeweiss. 2010. “Confounding Control in Healthcare Database Research: Challenges and Potential Approaches.” Medical Care 48 (6): S114–20. https://doi.org/10.1097/MLR.0b013e3181dbebe3.\n\n\nCole, Stephen R., and Constantine E. Frangakis. 2009. “The Consistency Statement in Causal Inference: A Definition or an Assumption?” Epidemiology 20 (1): 3–5. https://doi.org/10.1097/EDE.0b013e31818ef366.\n\n\nFogarty, Colin B., Mark E. Mikkelsen, David F. Gaieski, and Dylan S. Small. 2016. “Discrete Optimization for Interpretable Study Populations and Randomization Inference in an Observational Study of Severe Sepsis Mortality.” Journal of the American Statistical Association 111 (514): 447–58. https://doi.org/10.1080/01621459.2015.1112802.\n\n\nGreenland, Sander, Judea Pearl, and James M. Robins. 1999. “Causal Diagrams for Epidemiologic Research.” Epidemiology 10 (1): 37–48. https://www.jstor.org/stable/3702180.\n\n\nGreifer, Noah, and Elizabeth A. Stuart. 2021. “Choosing the Estimand When Matching or Weighting in Observational Studies.” arXiv:2106.10577 [Stat], June. https://arxiv.org/abs/2106.10577.\n\n\nHan, Shasha, and Xiao-Hua Zhou. 2023. “Defining Estimands in Clinical Trials: A Unified Procedure.” Statistics in Medicine, March, sim.9702. https://doi.org/10.1002/sim.9702.\n\n\nHernán, Miguel A, and James M. Robins. 2006. “Estimating Causal Effects from Epidemiological Data.” Journal of Epidemiology and Community Health (1979-) 60 (7): 578–86. http://www.jstor.org/stable/40795098.\n\n\nHernán, Miguel A, and S L Taubman. 2008. “Does Obesity Shorten Life? The Importance of Well-Defined Interventions to Answer Causal Questions.” International Journal of Obesity 32 (S3): S8–14. https://doi.org/10.1038/ijo.2008.82.\n\n\nHuber, Martin. 2015. “Causal Pitfalls in the Decomposition of Wage Gaps.” Journal of Business & Economic Statistics 33 (2): 179–91. https://doi.org/10.1080/07350015.2014.937437.\n\n\nImai, Kosuke, and David A. Van Dyk. 2004. “Causal Inference with General Treatment Regimes: Generalizing the Propensity Score.” Journal of the American Statistical Association 99 (467): 854–66. https://www.jstor.org/stable/27590455.\n\n\nKahan, Brennan C, Suzie Cro, Fan Li, and Michael O Harhay. 2023. “Eliminating Ambiguous Treatment Effects Using Estimands.” American Journal of Epidemiology, February, kwad036. https://doi.org/10.1093/aje/kwad036.\n\n\nKing, Gary, and Langche Zeng. 2006. “The Dangers of Extreme Counterfactuals.” Political Analysis 14 (2): 131–59. https://doi.org/10.1093/pan/mpj004.\n\n\nMatthay, Ellicott C., Erin Hagan, Laura M. Gottlieb, May Lynn Tan, David Vlahov, Nancy E. Adler, and M. Maria Glymour. 2020. “Alternative Causal Inference Methods in Population Health Research: Evaluating Tradeoffs and Triangulating Evidence.” SSM - Population Health 10 (April): 100526. https://doi.org/10.1016/j.ssmph.2019.100526.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized Experiments.” Journal of the American Statistical Association 102 (477): 191–200. https://doi.org/10.1198/016214506000001112.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1093/biomet/70.1.41.\n\n\nTchetgen, Eric J Tchetgen, and Tyler J VanderWeele. 2012. “On Causal Inference in the Presence of Interference.” Statistical Methods in Medical Research 21 (1): 55–75. https://doi.org/10.1177/0962280210386779.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34 (3): 211–19. https://doi.org/10.1007/s10654-019-00494-6.\n\n\nWestreich, Daniel, and Stephen R. Cole. 2010. “Invited Commentary: Positivity in Practice.” American Journal of Epidemiology 171 (6): 674–77. https://doi.org/10.1093/aje/kwp436."
  },
  {
    "objectID": "running.html",
    "href": "running.html",
    "title": "2  Basic Steps",
    "section": "",
    "text": "In this section, we describe how to run a propensity score analysis. First, we describe the steps and the choices one must make in each step, and then we provide some examples for how to run the described analysis using R.\nThe basic steps of the analysis are as follows:\n\nConceptual step - Selecting the desired estimand, assessing whether the assumptions can be met, and selecting the variables that must be adjusted for.\nConditioning step - Performing the matching, weighting, or subclassification, which may involve estimating a propensity score or selecting parameters that govern the conditioning specification (e.g., the number of matches if doing matching).\nAssessing quality - Assessing to what degree the conditioning step was successful in achieving balance in the distribution of confounders while retaining the target estimand and maintaining precision of the estimate.\nRespecification - If quality of the conditioning can be improved, respecifying the conditioning step and assessing quality again. This is done repeatedly until a good quality specification is found.\nEstimate the treatment effect - Using the output of the final conditioning specification, estimating the treatment effect in a robust way that optimizes precision.\nAssess sensitivity - Assessing the degree to which unmeasured confounding could affect inferences and deciding whether such confounding is a realistic threat.\n\nWe will go through these steps in the next sections."
  },
  {
    "objectID": "conceptual.html#choosing-the-estimand",
    "href": "conceptual.html#choosing-the-estimand",
    "title": "3  Conceptual aspects of the analysis",
    "section": "3.1 Choosing the estimand",
    "text": "3.1 Choosing the estimand\nFirst, one needs to decide on the desired estimand. The choice of which portion of the population to target should be driven primarily by the intended use of the study, even if such a use is not explicitly stated in the paper. For example, a study may examine the causal effect of the choice between traditional surgery and minimally invasive surgery on recovery time. The implied policies being compared are a policy in which traditional surgery is always used and a policy in which minimally invasive surgery is always used (even if the study does not intend to make such a sweeping policy). If a study is examining the effects of smoking on throat cancer, the implied policies being compared are a policy in which smokers continue smoking and one in which smokers no longer smoke. However, it may not be coherent to investigate a policy in which non-smokers are made to smoke.\nThe implied policy affects which estimand is chosen; for the first research question, the ATE might be of interest if there are few existing medical reasons to prefer one form of surgery to other, and the ATO might of interest if, for most patients, the choice is well understood, but there are some patients for whom a definitive choice is less clear (i.e., those at equipoise). For the second research question, which examines a harmful exposure, the ATT is more of interest because it corresponds to a realistic policy, i.e., getting smokers to quit. To help match the research question to the estimand, see Greifer and Stuart (2021).\nThe target population of the estimand is not the only choice one must make. As previously described, one must consider the time scale of the outcome, the scale the treatment effect is to be measured, how the treatment should be defined, etc. These choices are completely separate from the method of analysis but must be chosen beforehand so the analysis can proceed. They should be driven primarily by substantive concerns, e.g., at what time scale the treatment is supposed to have an effect, the effect measure that will make the most sense for clinicians and stakeholders, the implied policy that is most realistic or informative, etc. In some cases, though, proceeding through the analysis will reveal that some of the chosen options cannot be upheld, and they can be respecified in a dynamic process that maximizes the utility of the resulting research while respecting the degree of information supplied in the data."
  },
  {
    "objectID": "conceptual.html#assessing-assumptions",
    "href": "conceptual.html#assessing-assumptions",
    "title": "3  Conceptual aspects of the analysis",
    "section": "3.2 Assessing assumptions",
    "text": "3.2 Assessing assumptions\nAfter selecting a quantity to target, one must assess the assumptions required for causality, described previously in Chapter 1. One must decide whether the data available are sufficient to satisfy the backdoor criterion, and, if so, which variables must be adjusted for and which must not be. For example, there may be a strong predictor of the outcome that is affected by the treatment; this variable should not be adjusted for because doing so would induce bias (Elwert and Winship 2014). There may be a well-known confounder that is missing from the data because it was not collected in the database used; in that case, it must be made clear that the resulting estimates have no valid causal interpretation (and therefore little utility for practice) or the research design can be changed to use a method that requires assumptions other than satisfaction of the backdoor criterion like instrumental variables analysis (Hernán and Robins 2006).\nOne must assess whether consistency is met, i.e., whether there are no unmeasured versions of treatment. For example, if treatment is a binarized version of a truly continuous or multi-category variable, then the causal effect is not well defined and will not generalize to other categorization strategies. Using BMI as a treatment can incur this problem; defining treatment as having low vs. high BMI as determined by a cutoff assumes there is no difference among those with BMI just above the cutoff point and those with BMI far above the cutoff point. Instead, BMI might be considered as a continuous treatment, with methods appropriate for that treatment type used.\nOnce the assumptions are determined to be met, one can proceed with the analysis of the data.\n\n\n\n\nElwert, Felix, and Christopher Winship. 2014. “Endogenous Selection Bias: The Problem of Conditioning on a Collider Variable.” Annual Review of Sociology 40 (1): 31–53. https://doi.org/10.1146/annurev-soc-071913-043455.\n\n\nGreifer, Noah, and Elizabeth A. Stuart. 2021. “Choosing the Estimand When Matching or Weighting in Observational Studies.” arXiv:2106.10577 [Stat], June. https://arxiv.org/abs/2106.10577.\n\n\nHernán, Miguel A., and James M. Robins. 2006. “Instruments for Causal Inference: An Epidemiologist’s Dream?” Epidemiology 17 (4): 360–72. https://doi.org/10.1097/01.ede.0000222409.00878.37."
  },
  {
    "objectID": "conditioning.html#sec-weighting",
    "href": "conditioning.html#sec-weighting",
    "title": "4  Conditioning: matching and weighting",
    "section": "4.1 Weighting",
    "text": "4.1 Weighting\nWeighting involves estimating a weight for each unit in the sample such that, in the weighted sample, the covariates are balanced. Effect estimation proceeds by computing the weighted outcome means in each treatment group and computing their contrast or fitting a weighted outcome regression model with the treatment as a predictor. Weighting can be used to target any estimand, but the formulas for how the weights are computed depends on the estimand desired. Balancing weights function similarly to sampling weights; while sampling weights shift a sample to be representative of a national population, balancing weights shift each treatment group to resemble each other and a target population.\nThe most basic form of weighting is propensity score weighting for the ATE, also known as inverse probability of treatment weighting (IPTW). The weights have the following formula:\n\\[\nw_{ATE, i} = \\begin{cases}\n  1/p_i, & \\text{if } A_i = 1, \\\\\n  1/(1-p_i), & \\text{if } A_i = 0\n\\end{cases}\n\\]\nwhere \\(p_i\\) is an individual’s propensity score and \\(A_i\\) is an individual’s treatment value, 1 if treated and 0 if untreated. They are known as “inverse probability” weights because the weights are equal to the inverse of the probability of receiving the treatment actually received. IPTW weights shift the distributions of both the treated and untreated groups to resemble that of the full sample and therefore each other.\nPropensity score weighting for the ATT is also known as standardized mortality ratio weighting, and the weights have the following formula:\n\\[\nw_{ATT, i} = p_i \\times w_{ATE, i} = \\begin{cases}\n  1, & \\text{if } A_i = 1, \\\\\n  p_i/(1-p_i), & \\text{if } A_i = 0\n\\end{cases}\n\\]\nWeights for the ATT shift the distribution of the untreated units to resemble that of the treated units and leave the treated units untouched.\nPropensity score weighting for the ATO is also known as overlap weighting, and the weights have the following formula:\n\\[\nw_{ATO, i} = p_i(1-p_i) \\times w_{ATE, i} = \\begin{cases}\n  1-p_i, & \\text{if } A_i = 1, \\\\\n  p_i, & \\text{if } A_i = 0\n\\end{cases}\n\\]\nOverlap weights upweight units most like those in the other treatment group. Though there are other methods to compute weights that target an overlap sample (e.g., the “matching weights” of L. Li and Greene (2013)), overlap weights tend to outperform them and produce a weighted sample with the most precision of any propensity score weights.\nA choice that researchers must make when using propensity score weighting is how to estimate the propensity score. This choice affects the properties of the weights (i.e., the balance they induces and the precision in the weighted sample). The most common method is a logistic regression of the treatment on the covariates. Other popular methods involve machine learning methods like generalized boosted modeling (GBM) (McCaffrey, Ridgeway, and Morral 2004), Bayesian additive regression trees (BART) (Hill, Weiss, and Zhai 2011), and Super Learner (Alam, Moodie, and Stephens 2019), though any model that produces predicted class probabilities can be used to estimate propensity scores. Often, versions of these methods incorporate balance optimization into the estimation of the weights; for example, a popular implementation of GBM chooses the value of a tuning parameter as that which minimizes an imbalance statistic (McCaffrey, Ridgeway, and Morral 2004). Logistic regression has a particular benefit when using overlap weights: the covariate means will be exactly balanced between the treatment groups.\nMany modern methods skip the step of estimating a propensity score and estimate the weights directly. Examples of this approach include entropy balancing (Hainmueller 2012; Zhao and Percival 2017), stable balancing weights (Zubizarreta 2015), and energy balancing (Huling and Mak, n.d.). This distinction is explored in detail by Chattopadhyay, Hase, and Zubizarreta (2020). A popular weighting method, covariate balancing propensity score (CBPS) weighting, combines optimization and logistic regression-based propensity score estimation (Imai and Ratkovic 2014) (though this does not necessarily confer any benefits over methods that don’t estimate a propensity score (Y. Li and Li 2021)). These optimization-based methods often exactly or approximately balance features of the distribution of covariates while retaining precision in the weighted sample, making them highly effective1.\nWeighting methods sometimes yield “extreme” weights, i.e., a few weights that take on a much larger value than the others and dominate the analysis, which can make balance worse and reduce precision. This can occur especially when propensity score weighting for the ATE, as small propensity scores in the treated group and large propensity scores in the untreated group cause weights to be large when inverted. One approach to dealing with extreme weights is to trim the weights, which can involve either removing units with large weights or extreme propensity scores or setting the value of their weights to a smaller value (winsorizing). These methods often change the estimand from the ATE, in which case the ATO should be targeting using overlap weights instead."
  },
  {
    "objectID": "conditioning.html#sec-matching",
    "href": "conditioning.html#sec-matching",
    "title": "4  Conditioning: matching and weighting",
    "section": "4.2 Matching",
    "text": "4.2 Matching\nMatching involves dropping or reorganizing units into strata such that the remaining sample is balanced (Greifer and Stuart 2021a). Examples include pair matching, pure subset selection (in which units are dropped from the sample but no pairing occurs), and subclassification, among others. The outputs of a matching method are a set of matching weights and, if pairing or stratification is done, pair or stratum membership for each unit. Matching weights function identically to propensity score weights as described above; indeed, matching can be seen as a restricted form of weighting, where that restriction can sometimes afford benefits in terms of robustness and precision. Stuart (2010) provides an excellent introduction to matching.\nBelow, we briefly describe the two major forms of matching, stratification and subset selection (including pair matching).\n\n4.2.1 Stratification\nStratification simply involves assigning units into strata such that, within strata, the covariates are balanced between treatment groups. The most straightforward method of stratification is exact matching, in which sets of units with identical covariate values are placed into strata based on those values. Any units with no exact matches in the other treatment group are dropped. The remaining sample will be exactly balanced on all included covariates. In practice, continuous variables or categorical variables with many categories make exact matching impossible. One alternative is coarsened exact matching (CEM) (Iacus, King, and Porro 2012, 2011), which is simply exact matching on coarsened version of the covariates. The degree of coarsening is controlled by the researcher to strike a balance between discarding units with no matches and ensuring the units within strata are relatively homogeneous in the covariates.\nAnother alternative is propensity score subclassification (Rosenbaum and Rubin 1984), in which units are placed into strata based on their propensity score values. The number of strata is decided by the researcher; although early literature recommends as few as 5 subclasses, it is always best to try larger numbers of subclass to find the one that yields the highest quality matches, which can sometimes be in the hundreds depending on the sample size (Desai et al. 2017).\nThe result of stratification is a set of stratification weights. These are computed as follows: 1) compute the proportion of units in each stratum that are treated, 2) for each unit, assign to it this proportion in its stratum as a new stratum “propensity score”, and 3) apply the propensity score weighting formulas above to the new stratum propensity score using the formula that corresponds to the desired estimand. In this way, stratification is a form of propensity score weighting in which the weights are estimated using a multi-step procedure rather than directly from the propensity scores. This method is known as marginal mean weighting through stratification (MMWS) (Hong 2010) or fine stratification (Desai et al. 2017). This blog post explains this idea in more detail.\n\n\n4.2.2 Subset selection and pair matching\nSubset selection involves taking a subset of units from the original sample and dropping the rest, ideally in such a way that the remaining sample is balanced on the covariates. The most common method of subset selection is pair matching, in which treated units are paired with untreated units, and any unpaired units are dropped. There are a number of ways to customize pair matching to improve the balance and precision of the resulting sample:\n\nThe distance measure use to compute the closeness between units. The most common measure is the propensity score difference between each treated and untreated unit. Other distances include the Mahalanobis distance and its robust variant and the scaled Euclidean distance, which are computed from the covariates directly and do not require a propensity score, though a propensity score can be added as an additional covariate in computing them. A powerful optimization-based matching method called genetic matching adjusts elements of the distance measure used in order to optimize the balance of the resulting sample (Diamond and Sekhon 2013). The best distance measure to use will depend on the unique features of the dataset (Ripollone et al. 2018), so several should be tried, though genetic matching automates this process.\nThe number of matches unit receives. Each treated unit can receive one or more control units as a match, and this number can be chosen by the researcher. For example, one can request 2:1 matching instead of 1:1 matching, which increases the size of the resulting sample but may worsen balance because worse matches are being included. The number of matches each unit receives can be fixed across all units or varied (Ming and Rosenbaum 2000).\nWhether matching is done with or without replacement. One can choose whether untreated units can be reused as matches for multiple treated units. Matching with replacement often yields better balance and eliminates the effect of who gets matched first on the resulting sample. Matching with replacement can yield imprecision in the effect estimate when the same untreated unit is matched many times; the number of times each untreated unit can be matched can be limited by the researcher. Inference after matching with replacement can be more challenging than when matching without replacement2.\nThe order of matches. When matching without replacement, the order that treated units are matched matters. There are a variety of ways one can specify this order with varying evidence supporting each choice (Rubin 1973; Austin 2014), so it is best to try various orders. An alternative is to use optimal pair matching (Hansen and Klopfer 2006; Gu and Rosenbaum 1993), which optimizes a global distance criterion.\nCalipers and exact matching constraints. A caliper is a limit on how far two units can be before they are disallowed from being matched. Using calipers can improve balance but decrease precision because additional units are discarded (i.e., those without any matches within the caliper) (Austin 2014). It is very common to place a caliper on the propensity score, though it is also possible to place calipers on covariates directly. Though there has been some research into optimal caliper widths (Austin 2011), the best caliper will depend on the unique features of the dataset, and so many should be tried and evaluated. An exact matching constraint requires that two unit have identical values of the given covariate in order to be allowed to be matched. When calipers or exact matching constraints are used, balance often improves (sometimes dramatically), but discarding treated units changes the estimand to the ATO, which may not be desired (Greifer and Stuart 2021b; Rosenbaum and Rubin 1985). Applying calipers can also make balance worse if good balance has already been achieved without them (King and Nielsen 2019).\n\nThere are also methods of subset selection without pairing, such as cardinality and profile matching (Zubizarreta, Paredes, and Rosenbaum 2014; Cohn and Zubizarreta 2022). These use optimization to find the largest matched sample that satisfies balance constraints set by the user and are starting to see broader use in medical research (e.g., Niknam and Zubizarreta 2022; Fortin and Schuemie 2022).\n\n\n4.2.3 Full matching\nFull matching is an effective matching method that is somewhat of a cross between pair matching and stratification (Stuart and Green 2008; Hansen and Klopfer 2006). Every unit in the sample is assigned to a stratum as with stratification methods, but the strata are formed based on the pairwise distances between units. Full matching tends to outperform other matching methods and can be customized in many of the same ways (Austin and Stuart 2015, 2017b). Variations of full matching often run much faster than other matching algorithms (Sävje, Higgins, and Sekhon 2021). Unlike other pair matching methods, full matching can be used to estimate the ATT, ATC, or ATE. Full matching can also been seen as a alternative to IPTW that can be more robust to misspecification (Austin and Stuart 2017a)."
  },
  {
    "objectID": "conditioning.html#choosing-a-specification",
    "href": "conditioning.html#choosing-a-specification",
    "title": "4  Conditioning: matching and weighting",
    "section": "4.3 Choosing a specification",
    "text": "4.3 Choosing a specification\nWe explain how to choose among the variety of weighting and matching methods in Chapter 5. In short, the choice should depend on covariate balance, precision, and respect of the desired estimand. One does not need to choose a method and commit to it; one can instead try many, assess their quality, and move forward with effect estimation using only the best among those compared. However, this search can be shortened by using methods known to perform exceptionally well or that can be specified to respond to a researcher’s precise requirements.\nSome methods are more popular in certain fields; for example, medical research often uses matching, whereas epidemiological research often uses weighting. In some cases, the popularity of methods in certain fields reflects real substantive demands, but in many cases it simply reflects trends and cultures or the specific methods emphasized in training materials for students. For example, epidemiological training emphasizes the use of weighting over matching (Hernán and Robins 2020), even though they often serve the same purpose and perform equally well (Greifer and Stuart 2021a; Kush et al. 2022).\nOne key aspect to remember is that the basic or default method is almost never the best method and should not be used just because it is the most familiar or popular in a field. For example, 1:1 pair matching on the propensity score is a popular matching method in medical research, even though it is uniformly outperformed by genetic matching (Diamond and Sekhon 2013) and very often performs worse than methods that are no more difficult to use, such as full matching (Austin and Stuart 2015) and cardinality matching (Visconti and Zubizarreta 2018). Similarly, propensity score weighting often performs worse than modern optimization-based methods like entropy balancing (Hainmueller 2012).\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2006. “Large Sample Properties of Matching Estimators for Average Treatment Effects.” Econometrica 74 (1): 235–67. https://doi.org/10.1111/j.1468-0262.2006.00655.x.\n\n\n———. 2016. “Matching on the Estimated Propensity Score.” Econometrica 84 (2): 781–807. https://doi.org/10.3982/ECTA11293.\n\n\nAlam, Shomoita, Erica E. M. Moodie, and David A. Stephens. 2019. “Should a Propensity Score Model Be Super? The Utility of Ensemble Procedures for Causal Adjustment.” Statistics in Medicine 38 (9): 1690–1702. https://doi.org/10.1002/sim.8075.\n\n\nAustin, Peter C. 2011. “Optimal Caliper Widths for Propensity-Score Matching When Estimating Differences in Means and Differences in Proportions in Observational Studies.” Pharmaceutical Statistics 10 (2): 150–61. https://doi.org/10.1002/pst.433.\n\n\n———. 2014. “A Comparison of 12 Algorithms for Matching on the Propensity Score.” Statistics in Medicine 33 (6): 1057–69. https://doi.org/10.1002/sim.6004.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2015. “Optimal Full Matching for Survival Outcomes: A Method That Merits More Widespread Use.” Statistics in Medicine 34 (30): 3949–67. https://doi.org/10.1002/sim.6602.\n\n\n———. 2017a. “The Performance of Inverse Probability of Treatment Weighting and Full Matching on the Propensity Score in the Presence of Model Misspecification When Estimating the Effect of Treatment on Survival Outcomes.” Statistical Methods in Medical Research 26 (4): 1654–70. https://doi.org/10.1177/0962280215584401.\n\n\n———. 2017b. “Estimating the Effect of Treatment on Binary Outcomes Using Full Matching on the Propensity Score.” Statistical Methods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nChattopadhyay, Ambarish, Christopher H. Hase, and José R. Zubizarreta. 2020. “Balancing Vs Modeling Approaches to Weighting in Practice.” Statistics in Medicine 39 (24): 3227–54. https://doi.org/10.1002/sim.8659.\n\n\nCohn, Eric R., and José R. Zubizarreta. 2022. “Profile Matching for the Generalization and Personalization of Causal Inferences.” Epidemiology 33 (5): 678. https://doi.org/10.1097/EDE.0000000000001517.\n\n\nDesai, Rishi J., Kenneth J. Rothman, Brian .T Bateman, Sonia Hernandez-Diaz, and Krista F. Huybrechts. 2017. “A Propensity-Score-Based Fine Stratification Approach for Confounding Adjustment When Exposure Is Infrequent:” Epidemiology 28 (2): 249–57. https://doi.org/10.1097/EDE.0000000000000595.\n\n\nDiamond, Alexis, and Jasjeet S. Sekhon. 2013. “Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.” Review of Economics and Statistics 95 (3): 932945. https://doi.org/10.1162/REST_a_00318.\n\n\nFortin, Stephen P., and Martijn Schuemie. 2022. “Indirect Covariate Balance and Residual Confounding: An Applied Comparison of Propensity Score Matching and Cardinality Matching.” Pharmacoepidemiology and Drug Safety 31 (12): 1242–52. https://doi.org/10.1002/pds.5510.\n\n\nGreifer, Noah, and Elizabeth A Stuart. 2021a. “Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist’s Toolbox.” Epidemiologic Reviews, June, mxab003. https://doi.org/10.1093/epirev/mxab003.\n\n\nGreifer, Noah, and Elizabeth A. Stuart. 2021b. “Choosing the Estimand When Matching or Weighting in Observational Studies.” arXiv:2106.10577 [Stat], June. https://arxiv.org/abs/2106.10577.\n\n\nGu, Xing Sam, and Paul R. Rosenbaum. 1993. “Comparison of Multivariate Matching Methods: Structures, Distances, and Algorithms.” Journal of Computational and Graphical Statistics 2 (4): 405. https://doi.org/10.2307/1390693.\n\n\nHainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHansen, Ben B, and Stephanie Olsen Klopfer. 2006. “Optimal Full Matching and Related Designs via Network Flows.” Journal of Computational and Graphical Statistics 15 (3): 609–27. https://doi.org/10.1198/106186006X137047.\n\n\nHernán, Miguel A, and James M Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC. https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf.\n\n\nHill, Jennifer, and Jerome P. Reiter. 2006. “Interval Estimation for Treatment Effects Using Propensity Score Matching.” Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHill, Jennifer, Christopher Weiss, and Fuhua Zhai. 2011. “Challenges With Propensity Score Strategies in a High-Dimensional Setting and a Potential Alternative.” Multivariate Behavioral Research 46 (3): 477–513. https://doi.org/10.1080/00273171.2011.570161.\n\n\nHong, Guanglei. 2010. “Marginal Mean Weighting Through Stratification: Adjustment for Selection Bias in Multilevel Data.” Journal of Educational and Behavioral Statistics 35 (5): 499–531. https://doi.org/10.3102/1076998609359785.\n\n\nHuling, Jared D., and Simon Mak. n.d. “Energy Balancing of Covariate Distributions.” https://doi.org/10.48550/arXiv.2004.13962.\n\n\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2011. “Multivariate Matching Methods That Are Monotonic Imbalance Bounding.” Journal of the American Statistical Association 106 (493): 345–61. https://doi.org/10.1198/jasa.2011.tm09599.\n\n\n———. 2012. “Causal Inference Without Balance Checking: Coarsened Exact Matching.” Political Analysis 20 (1): 1–24. https://doi.org/10.1093/pan/mpr013.\n\n\nImai, Kosuke, and Marc Ratkovic. 2014. “Covariate Balancing Propensity Score.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (1): 243263. https://doi.org/10.1111/rssb.12027.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis, May, 1–20. https://doi.org/10.1017/pan.2019.11.\n\n\nKush, Joseph M., Elise T. Pas, Rashelle J. Musci, and Catherine P. Bradshaw. 2022. “Covariate Balance for Observational Effectiveness Studies: A Comparison of Matching and Weighting.” Journal of Research on Educational Effectiveness 0 (0): 1–24. https://doi.org/10.1080/19345747.2022.2110545.\n\n\nLi, Liang, and Tom Greene. 2013. “A Weighting Analogue to Pair Matching in Propensity Score Analysis.” The International Journal of Biostatistics 9 (2). https://doi.org/10.1515/ijb-2012-0030.\n\n\nLi, Yan, and Liang Li. 2021. “Propensity Score Analysis Methods with Balancing Constraints: A Monte Carlo Study.” Statistical Methods in Medical Research 30 (4): 1119–42. https://doi.org/10.1177/0962280220983512.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. “Propensity Score Estimation With Boosted Regression for Evaluating Causal Effects in Observational Studies.” Psychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nMing, Kewei, and Paul R. Rosenbaum. 2000. “Substantial Gains in Bias Reduction from Matching with a Variable Number of Controls.” Biometrics 56 (1): 118–24. https://doi.org/10.1111/j.0006-341X.2000.00118.x.\n\n\nNiknam, Bijan A., and Jose R. Zubizarreta. 2022. “Using Cardinality Matching to Design Balanced and Representative Samples for Observational Studies.” JAMA 327 (2): 173–74. https://doi.org/10.1001/jama.2021.20555.\n\n\nRipollone, John E., Krista F. Huybrechts, Kenneth J. Rothman, Ryan E. Ferguson, and Jessica M. Franklin. 2018. “Implications of the Propensity Score Matching Paradox in Pharmacoepidemiology.” American Journal of Epidemiology 187 (9): 1951–61. https://doi.org/10.1093/aje/kwy078.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1984. “Reducing Bias in Observational Studies Using Subclassification on the Propensity Score.” Journal of the American Statistical Association 79 (387): 516–24. https://doi.org/10.2307/2288398.\n\n\n———. 1985. “The Bias Due to Incomplete Matching.” Biometrics 41 (1): 103–16. https://doi.org/10.2307/2530647.\n\n\nRubin, Donald B. 1973. “Matching to Remove Bias in Observational Studies.” Biometrics 29 (1): 159–83. https://doi.org/10.2307/2529684.\n\n\nSävje, Fredrik, Michael J. Higgins, and Jasjeet S. Sekhon. 2021. “Generalized Full Matching.” Political Analysis 29 (4): 423–47. https://doi.org/10.1017/pan.2020.32.\n\n\nStuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science 25 (1): 1–21. https://doi.org/10.1214/09-STS313.\n\n\nStuart, Elizabeth A., and Kerry M. Green. 2008. “Using Full Matching to Estimate Causal Effects in Nonexperimental Studies: Examining the Relationship Between Adolescent Marijuana Use and Adult Outcomes.” Developmental Psychology, New methods for new questions in developmental psychology, 44 (2): 395–406. https://doi.org/10.1037/0012-1649.44.2.395.\n\n\nVisconti, Giancarlo, and José R. Zubizarreta. 2018. “Handling Limited Overlap in Observational Studies with Cardinality Matching.” Observational Studies 4 (1): 217–49. https://doi.org/10.1353/obs.2018.0012.\n\n\nZhao, Qingyuan, and Daniel Percival. 2017. “Entropy Balancing Is Doubly Robust.” Journal of Causal Inference 5 (1). https://doi.org/10.1515/jci-2016-0010.\n\n\nZubizarreta, José R. 2015. “Stable Weights That Balance Covariates for Estimation with Incomplete Outcome Data.” Journal of the American Statistical Association 110 (511): 910–22. https://doi.org/10.1080/01621459.2015.1023805.\n\n\nZubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014. “Matching for Balance, Pairing for Heterogeneity in an Observational Study of the Effectiveness of for-Profit and Not-for-Profit High Schools in Chile.” The Annals of Applied Statistics 8 (1): 204–31. https://doi.org/10.1214/13-AOAS713."
  },
  {
    "objectID": "conditioning.html#footnotes",
    "href": "conditioning.html#footnotes",
    "title": "4  Conditioning: matching and weighting",
    "section": "",
    "text": "A common theme in many areas of statistics is that newer methods perform better than older methods in general. However, those newer methods are often less well studied and opaque or unheard of by applied researchers. Their absence from the applied literature and the popularity of older, more basic methods is not an indication that the basic methods should be preferred; rather, it often reflects the fear or ignorance by researchers and reviewers of newer, better performing methods.↩︎\nAlthough much early research into statistical inference for matching was done for matching with replacement, the inference methods required highly specific uses of matching and complicated estimators of standard errors (Abadie and Imbens 2006, 2016). The most applicable methods of inference for matching with replacement rely on simulation evidence and are only approximations (Hill and Reiter 2006).↩︎"
  },
  {
    "objectID": "assessing.html#sec-balance",
    "href": "assessing.html#sec-balance",
    "title": "5  Assessing quality",
    "section": "5.1 Covariate Balance",
    "text": "5.1 Covariate Balance\nBalance is the degree to which the treatment groups resemble each other on the covariates. A fully balanced sample is one in which the distributions of covariates in the treatment groups are identical. When balance is achieved, bias due to confounding by the observed confounders will be eliminated. Balance is central to the use of propensity score methods (Ben-Michael et al. 2021); indeed, the entire point of using these methods is to achieve balance. The primary benefit of randomization is that confounders (both observed and unobserved) are balanced by design (in expectation); propensity score methods seek to mimic this quality by adjusting the sample so that balance is achieved (though only on the observed covariates).\nIn theory, conditioning on the propensity score via matching or weighting should yield balance in the sample (Rosenbaum and Rubin 1983); this is the key balancing property of propensity scores and why they are popular in the first place. However, in practice, they don’t always yield balance, which can be due to a number of factors, including that the propensity score model (if one is used) is misspecified, the conditioning method is not suited for the data used, or balance fundamentally cannot be achieved while retaining the desired estimand. Therefore, it is critical that balance be assessed before moving forward with effect estimation.\nThis is what Ho et al. (2007) describe as the “propensity score tautology”: conditioning on a good propensity score yields balance in the sample, but whether a propensity score is good depends on whether it yields balance. The implication of this is that we cannot rely on the theoretical balancing properties of the propensity score; we need to empirically ensure that the conditioning specification is doing its job.\nFull balance means balance not just on individual covariates but on the full, joint distribution of covariates. For example, it may not be enough to have the same proportion of women and the same proportion of Black patients in the treatment groups; full balance requires the same proportion of Black women, non-Black women, Black men, and non-Black men in the treatment groups. Full balance can be hard to achieve and hard to assess, so most balance assessment methods focus on the “marginal” (i.e., single-covariate) distributions; if balance is not achieved on individual covariates, it is not achieved on the full joint distribution either.\nBalance can be assessed graphically or numerically, and among numerical assessments, there are univariate statistics (one covariate at a time) and multivariate statistics (multiple covariates at a time).\n\n5.1.1 Graphical balance assessment\nThe most straightforward assessment of whether the distribution of a covariates is the same in the two treatment groups is simply to plot the distributions of the covariates and note any differences. Examples of distribution plots include histograms, kernel density plots, empirical cumulative distribution function (eCDF) plots, and bar graphs (for categorical variables). Dramatic differences in these plots suggests severe imbalance that must be corrected.\nSee below fo an example taken from the documentation for the R package cobalt. The top panel shows the distributions of age in the treated and untreated groups, on which some imbalance remains because the distributions are not identical.\n\n\n\n\n\nIn the bottom panel, the distribution of categories of race is almost identical between the treated and untreated groups, indicating good balance on this covariate.\n\n\n\n\n\n\n\n5.1.2 Univariate balance statistics\nUnivariate balance statistics describe the difference in the distributions of one covariate at a time numerically. For binary covariates, the difference in the proportion of each category across treatment groups is an example of univariate balance statistic (called the raw difference in proportion). For continuous variables, several univariate balance statistics are commonly used:\n\nStandardized mean difference (SMD): \\(\\frac{\\bar{x}_1-\\bar{x}_0}{s}\\), where \\(s\\) is a measure of spread computed in the unadjusted sample. The SMD does not depend on the scale of the covariate and only assess differences in the covariate means, not other features of the distributions. Ideally these are as small as possible (Ho et al. 2007), though some authors recommend that they should be below .1. SMDs are the most commonly reported balance statistic.\nVariance ratios: \\(s^2_1/s^2_0\\). This complements the SMD by assessing balance on the variability of the distribution, not just the means. Ideally these are as close to 1 as possible.\nKolmogorov-Smirnov (KS) statistics: \\(\\text{max} \\left( |F_1(x) - F_0(x)| \\right)\\), where \\(F(x)\\) is the eCDF of \\(x\\). This assesses balance on the entire marginal distribution of the covariate, not just the means or variability. Ideally these are as small as possible.\n\nThese statistics should also be computed not just on each covariate, but also on transformations of it, such as the square or cube or interactions between covariates (Austin and Stuart 2015).\nBalance should not be assessed on the propensity score itself; not only is this a conceptually invalid way to assess balance because a propensity score can only be considered useful when it balances the covariates (Ho et al. 2007), empirically there is no association between balance on the propensity score and low bias in the effect estimate (Stuart, Lee, and Leacy 2013). The propensity score is sometimes used a heuristic to assess overlap (e.g., to assess whether the failure of propensity score weighting or matching is due to large differences in the propensity score distribution between groups) and to assess why a matching method might be failing (e.g., if nearest neighbor matching cannot even balance the propensity scores, it has little hope for balancing the covariates).\n\n\n5.1.3 Multivariate balance statistics\nA multivariate balance statistic is a single (scalar) number that attempts to summarize balance for a sample. They are used less often in assessing balance and more to compare balance between conditioning specifications. Examples include the largest or average absolute SMD across covariates and the largest or average KS statistic across covariates. Some multivariate balance statistics attempt to assess balance not just on the marginal covariate distributions but on the joint distribution; examples include the \\(L_1\\) statistic (Iacus, King, and Porro 2011), which sums raw differences in proportion across a multi-dimensional histogram of the covariates, and the energy distance (Huling and Mak, n.d.), which computes the average distance between the joint covariate eCDFs. These statistics are not typically reported because their scale depends on the distribution and number of covariates, but they can be used to compare between specifications.\n\n\n5.1.4 Don’t use p-values\nOne might be tempted to use a hypothesis test to assess whether the distributions of the covariates differs between groups. For example, one might use the p-value on a t-test, chi-square test, Mann-Whitney U test, or KS test to test whether two distributions are the same. One should not do this to assess balance in observational studies, either before or after conditioning. There are several reasons to avoid p-values:\n\nHypothesis tests typically make reference to a population distribution, but balance is a quality of the sample\nHypothesis tests depend on the sample size, so shrinking your sample size (e.g., through subset selection) will appear to make balance better even if it makes it worse\nP-values suggest a significance threshold that might be invalid; \\(p = .1\\) might still indicate severe imbalance despite being nonsignificant (at the \\(\\alpha = .05\\) level).\n\nInstead, use the sample balance statistics described above. See Imai, King, and Stuart (2008) and Ali et al. (2015) for further explanation on why p-values should not be used or reported."
  },
  {
    "objectID": "assessing.html#sec-representativeness",
    "href": "assessing.html#sec-representativeness",
    "title": "5  Assessing quality",
    "section": "5.2 Representativeness",
    "text": "5.2 Representativeness\nRepresentativeness refers to how generalizable an effect estimate is to the population for whom one wants to make inferences. Even if the original sample is representative of a meaningful population (e.g., all patients with a given disease), matching or weighting may distort the sample in such a way as to change its representativeness. The reason this is so critical to consider is that different methods exchange representativeness for balance; for example, matching with a caliper or using overlap weights both change the target population to one that differs from the full sample or one of the treatment groups, but in doing so general dramatically improve balance, especially when the groups are very different from each other.\nRepresentativeness can also be thought of as balance between the adjusted sample and the original sample (or target population), and can be assessed in the same way, i.e., by examining the differences in covariate distributions between the adjusted sample and the target population. This is sometimes known as “three-way” balance, i.e., balance between the treated and control groups, between the treated group and the target population, and between the control group and the target population (Chan, Yam, and Zhang 2016). Typically, this is done by simply examining the distribution of covariates in the adjusted sample and in the target population and assessing heuristically whether they are close enough for the inference to be informative.\nIn some cases, representativeness is less important than ensuring groups are comparable, such as when attempting to discover whether treatment exists for any group rather than for a specific group (Mao, Li, and Greene 2018). In these cases, ensuring representativeness is less critical, but it remains important to understand for which population the estimated effect generalizes."
  },
  {
    "objectID": "assessing.html#sec-ess",
    "href": "assessing.html#sec-ess",
    "title": "5  Assessing quality",
    "section": "5.3 Effective sample size",
    "text": "5.3 Effective sample size\nMatching via subset selection discards units, which leaves the remaining sample smaller than the original sample. In some cases, many units are discarded, and the remaining sample is too small to detect effects. Thus, it is important that any matching method not only achieve balance (and representativeness) but also preserve sample size.\nIt might seem like weighting, full matching, or subclassification would prevent this problem because no units are discarded with these methods, but the weights resulting from these methods actually do induce a decrease in precision similar to shrinking the sample size. To capture this effect, the effective sample size (ESS) is used, which corresponds to the size of a hypothetical unweighted sample that carries as much precision as the weighted sample (Ridgeway 2006).\nThe ESS is computed in each treatment group as \\(\\left(\\sum_n{w}\\right)^2/\\sum_n{w_i^2}\\). When weights are scaled to have a mean of 1 in each treatment group, the ESS can be written as \\(\\frac{n}{1+\\text{Var}(w)}\\), which makes it clear that as the variability of the weights increases, the ESS decreases. When all weights are equal to 1 (e.g., prior to estimating propensity score weights), the ESS is equal to the original sample size.\nWhen a weighting or matching specification yields an ESS that is too small to use for inference, this suggests a problem with the conditioning specification that should be rectified. A low ESS typically arises from highly variable weights, which can occur when some units have very small or very large propensity scores; this is known as the problem of “extreme weights”, for which multiple solutions have been proposed. These solutions include using a different model to estimate the weights (e.g., using logistic regression instead of a classification tree), changing the estimand (e.g., targeting the ATO rather than the ATE), trimming the weights (which can change the estimand as well), or using a method that specifically aims to reduce the variability of the weights, like stable balancing weights (Zubizarreta 2015) or entropy balancing (Hainmueller 2012).\nOne often finds that balance, representativeness, and ESS pull in opposite directions, which requires users to manage a trade-off between bias, generalizability, and precision. For example, matching with a caliper often yields excellent balance but, by dropping units from both treatment groups, worsens representativeness and decreases the ESS. Propensity score weighting for the ATE may ensure representativeness, but it may be hard to achieve balance, and if one does, it may be that that balance comes at the cost of a decreased ESS due to extreme weights. There is no single method that can perfectly optimize all three, but the best methods allow the user to incorporate substantive information into making these trade-offs to best suit the data at hand and research question.\n\n\n\n\nAli, M. Sanni, Rolf H. H. Groenwold, Svetlana V. Belitser, Wiebe R. Pestman, Arno W. Hoes, Kit C. B. Roes, Anthonius de Boer, and Olaf H. Klungel. 2015. “Reporting of Covariate Selection and Balance Assessment in Propensity Score Analysis Is Suboptimal: A Systematic Review.” Journal of Clinical Epidemiology 68 (2): 122–31. https://doi.org/10.1016/j.jclinepi.2014.08.011.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2015. “Moving Towards Best Practice When Using Inverse Probability of Treatment Weighting (IPTW) Using the Propensity Score to Estimate Causal Treatment Effects in Observational Studies.” Statistics in Medicine 34 (28): 3661–79. https://doi.org/10.1002/sim.6607.\n\n\nBen-Michael, Eli, Avi Feller, David A. Hirshberg, and José R. Zubizarreta. 2021. “The Balancing Act in Causal Inference.” arXiv:2110.14831 [Stat], October. https://arxiv.org/abs/2110.14831.\n\n\nChan, Kwun Chuen Gary, Sheung Chi Phillip Yam, and Zheng Zhang. 2016. “Globally Efficient Non-Parametric Inference of Average Treatment Effects by Empirical Balancing Calibration Weighting.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 (3): 673–700. https://doi.org/10.1111/rssb.12129.\n\n\nHainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007. “Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.” Political Analysis 15 (3): 199–236. https://doi.org/10.1093/pan/mpl013.\n\n\nHuling, Jared D., and Simon Mak. n.d. “Energy Balancing of Covariate Distributions.” https://doi.org/10.48550/arXiv.2004.13962.\n\n\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2011. “Multivariate Matching Methods That Are Monotonic Imbalance Bounding.” Journal of the American Statistical Association 106 (493): 345–61. https://doi.org/10.1198/jasa.2011.tm09599.\n\n\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008. “Misunderstandings Between Experimentalists and Observationalists about Causal Inference.” Journal of the Royal Statistical Society. Series A (Statistics in Society) 171 (2): 481–502. https://doi.org/10.1111/j.1467-985X.2007.00527.x.\n\n\nMao, Huzhang, Liang Li, and Tom Greene. 2018. “Propensity Score Weighting Analysis and Treatment Effect Discovery.” Statistical Methods in Medical Research, June, 096228021878117. https://doi.org/10.1177/0962280218781171.\n\n\nRidgeway, Greg. 2006. “Assessing the Effect of Race Bias in Post-Traffic Stop Outcomes Using Propensity Scores.” Journal of Quantitative Criminology 22 (1): 1–29. https://doi.org/10.1007/s10940-005-9000-9.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1093/biomet/70.1.41.\n\n\nStuart, Elizabeth A., Brian K. Lee, and Finbarr P. Leacy. 2013. “Prognostic Score-Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.” Journal of Clinical Epidemiology 66 (8): S84. https://doi.org/10.1016/j.jclinepi.2013.01.013.\n\n\nZubizarreta, José R. 2015. “Stable Weights That Balance Covariates for Estimation with Incomplete Outcome Data.” Journal of the American Statistical Association 110 (511): 910–22. https://doi.org/10.1080/01621459.2015.1023805."
  },
  {
    "objectID": "respecification.html",
    "href": "respecification.html",
    "title": "6  Respecification",
    "section": "",
    "text": "If a given specification is not adequate in that balance is too poor, the effective sample size is too small, or the sample is no longer representative of the target population, one must respecify. Respecification can involve changing some aspect of the conditioning strategy, such as changing a parameter involved in the matching or weighting or changing the model used to estimate propensity scores. Because there are so many parameters that can be changed and they can be changed in so many ways, it is impossible to give a complete account of the best way to respecify. One should try many specifications, examining patterns in how making those changes improves the quality of the resulting sample. As long as the outcome is not involved in this process, doing so will not invalidate inferences made at the end.\nThere are some common tricks that can be used to nudge the respecification process in the right direction. Below are some common issues and some potential solutions.\n\nPoor balance as measured by SMDs: consider using an optimization-based method, like entropy balancing or cardinality matching or using a method that changes the estimand, like caliper matching or overlap weighting\nPoor balance beyond SMDs (e.g., on polynomial terms, variance ratios, or KS statistics): consider adding polynomial or interaction terms to the propensity score model; using a machine-learning method that flexibly models the propensity score; using an optimization-based method that balances the full distribution, like energy balancing; using coarsened exact matching to balance the full distribution approximately; or adding an exact matching constraint to a matching specification\nLow ESS: consider using a method to regularize the propensity score model (e.g., ridge or lasso regression); increasing the matching ratio; using an optimization-based method that maximizes the ESS (e.g., profile matching or stable balancing weights); relaxing the caliper (if used); trimming extreme weights; or using overlap weighting\nPoor representativeness: consider using a method that strongly respects the estimand (e.g., entropy balancing, not cardinality matching, caliper matching, or overlap weighting) or removing a caliper or exact matching restriction\n\nHaving broad experience with the variety of matching and weighting methods available makes this process quick. Fortunately, the software we recommend and use in the examples, the R packages MatchIt and WeightIt, make switching between various specifications easy.\nTo avoid endless respecification, it is a good idea to use methods designed to optimize the evaluation criteria in a simple way. Often, the oldest and most commonly used methods are the worst in that they perform poorly and require manual respecification to get right. For example, 1:1 propensity score matching with a caliper is the most commonly used propensity score method in medical research, but it is widely known to have many problems: it hampers representativeness because the caliper discards units from both treatment groups (Rosenbaum and Rubin 1985), it reduces the effective sample size by dropping many units from the sample, it can make balance worse when used thoughtlessly (King and Nielsen 2019), and it has many specification parameters that need to be adjusted arbitrarily (e.g., the propensity score model, caliper width, matching order, etc.). Another popular but old method, propensity score weighting, also has many problems, including inability to achieve balance, low ESS due to extreme weights, and reduced representativeness when measures are taken to rectify the other problems.\nMethods that consistently perform well include entropy balancing (Hainmueller 2012) and energy balancing (Huling and Mak, n.d.), as these ensure balance and representativeness without requiring major respecification. Entropy balancing guarantees exact balance as measured by the SMD, but it may be necessary to include other terms to fully balance the covariate distributions. Energy balancing balances the full covariate distribution, but can decrease ESS (though the trade-off between them can be managed with a single parameter). Though these methods are newer, they are beginning to see use in medical research (e.g., Bramante et al. 2022; Sharma et al. 2023) and should be the first line of defense when adjusting for confounders rather than poorly performing but older and more familiar methods.\n\n\n\n\nBramante, Carolyn T., Steven G. Johnson, Victor Garcia, Michael D. Evans, Jeremy Harper, Kenneth J. Wilkins, Jared D. Huling, et al. 2022. “Diabetes Medications and Associations with Covid-19 Outcomes in the N3C Database: A National Retrospective Cohort Study.” Edited by Surasak Saokaew. PLOS ONE 17 (11): e0271574. https://doi.org/10.1371/journal.pone.0271574.\n\n\nHainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHuling, Jared D., and Simon Mak. n.d. “Energy Balancing of Covariate Distributions.” https://doi.org/10.48550/arXiv.2004.13962.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis, May, 1–20. https://doi.org/10.1017/pan.2019.11.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1985. “The Bias Due to Incomplete Matching.” Biometrics 41 (1): 103–16. https://doi.org/10.2307/2530647.\n\n\nSharma, Mayur, Truong H. Do, Elise F. Palzer, Jared D. Huling, and Clark C. Chen. 2023. “Comparable Safety Profile Between Neuro-Oncology Procedures Involving Stereotactic Needle Biopsy (SNB) Followed by Laser Interstitial Thermal Therapy (LITT) and LITT Alone Procedures.” Journal of Neuro-Oncology 162 (1): 147–56. https://doi.org/10.1007/s11060-023-04275-w."
  },
  {
    "objectID": "estimating.html#the-effect-estimate",
    "href": "estimating.html#the-effect-estimate",
    "title": "7  Estimating the treatment effect",
    "section": "7.1 The effect estimate",
    "text": "7.1 The effect estimate\nThe simplest way to compute the treatment effect after matching or weighting is to compute the weighted mean of the outcome in each treatment group and contrast them to form the estimand of choice. For example, for a binary outcome of death by 6 months after propensity score weighting for the ATE, we can compute the weighted proportion of deaths in the treated group and the weighted proportion of deaths in the control group, which give us the expected potential outcomes under treatment and control for the full sampled population. We can compute the risk ratio by taking the ratio of these weighted proportions. Equally simple would be to run a weighted log-binomial regression of death by 6 months on the treatment and use the exponentiated coefficient on treatment as the estimated risk ratio.\n\n7.1.1 Adjusting for covariates\nFurther adjustment for covariates in the outcome model is a good idea as it improves precision at little to no cost and often reduces bias (though less so if the matching or weighting was effective)1. However, it is important to do so using a specific procedure that does not distort the target estimand. This procedure is called “g-computation” (Snowden, Rose, and Mortimer 2011), and its weighted variant is weighted g-computation (Vansteelandt and Keiding 2011), which will be the focus of this section.\nDo not use the coefficient on treatment as an estimate of the treatment effect when covariates are included in the model. This coefficient generally does not correspond to a valid marginal estimand and its interpretation depends on correct specification of the outcome model (and if we could do that, we wouldn’t need propensity score analysis in the first place). G-computation is a method of computing a marginal treatment effect from a model that preserves the estimand, is agnostic to the form of the model, and doesn’t require the model to be correctly specified to see the benefits of covariate adjustment.\nGenerally, g-computation works as follows:\n\nFit a model for the outcome given the treatment, covariates, and their interaction.\nGenerate predicted values from this model for all units after setting their treatment status to treated, regardless of their actual treatment status; these are the estimated potential outcomes under treatment.\nGenerate predicted values from this model for all units after setting their treatment status to control, regardless of their actual treatment status; these are the estimated potential outcomes under control.\nCompute the mean of the estimated potential outcomes under treatment and under control; these are the expected potential outcomes under treatment and control.\nContrast the expected potential outcomes to arrive at a marginal treatment effect estimate.\n\nWhen weights are included, we need to adjust the procedure slightly: the outcome model should be fit as a weighted model and the expected potential outcomes under each treatment level should be computed as weighted means. This ensures the balancing properties of the weights are employed and the treatment effect generalizes to the correct target population.\nOne of the primary benefits of g-computation is that the model and the estimand are completely divorced from each other, so one can fit the right model for the data regardless of which contrast is to be reported. For example, for a binary outcome, one can fit a logistic regression model but compute a risk difference using g-computation. Traditional approaches that rely on using the coefficient on treatment require the model to correspond to the desired contrast, which can yield bias if the model is not right for the data (e.g., a linear model for a binary outcome, which can produce predictions beyond 0 and 1).\nAnother benefit of g-computation is that the procedure is exactly the same no matter what model is used for the outcome, how that model is parameterized (i.e., whether polynomials or interactions are included in the model), or how the data was conditioned (i.e., matching or weighting). This obviates the need for considering the interpretability of the original model or preprocessing the data to ensure coefficients have valid interpretations, such as using contrast coding or centering. This also opens the door to using outcome models that are traditionally challenging to interpret (e.g., probit regression instead of logistic regression for binary outcomes, splines to capture nonlinear covariate effects, or multi-part models like zero-inflated Poisson models for count outcomes).\n\n\n7.1.2 Survival outcomes\nSurvival outcomes often require additional considerations because of censoring and the nature of the outcome. The most common method for describing the effect of a treatment on a survival outcome is the hazard ratio, which corresponds to the exponentiated coefficient in a Cox proportional hazards model. Hazard ratios carry a number of problems, which include that the hazard ratio is confounded, even in randomized trials, after a single event occurs (Hernán 2010); the hazard ratio is noncollapsible, meaning its value differs when considering individuals vs. the population; and the proportional hazards assumption is almost never satisfied, making the Cox model coefficient correspond to an ambiguously weighted average of the non-proportional time-specific hazards (Stensrud and Hernán 2020). Other estimands have been proposed that avoid the problems with hazard ratios and Cox models, which often are computed from the Kaplan-Maier estimate of the survival function, such as the restricted mean survival time (Pak et al. 2017) or risk difference at pre-specified follow-up intervals (Cafri and Austin 2023).\nFor many of these methods, weighted versions are available that can incorporate weights resulting from matching or weighting. Including covariates in the outcome model while maintaining a marginal estimand is often less straightforward, and methods for doing so are less developed or accessible, so we recommend simply using the weighted versions with treatment as the sole predictor or grouping variable."
  },
  {
    "objectID": "estimating.html#confidence-intervals-and-standard-errors",
    "href": "estimating.html#confidence-intervals-and-standard-errors",
    "title": "7  Estimating the treatment effect",
    "section": "7.2 Confidence intervals and standard errors",
    "text": "7.2 Confidence intervals and standard errors\nCare needs to be taken when computing confidence intervals (CIs) and standard errors (SEs), as these may need to take into account multiple sources of variation, including from estimation of the propensity scores or weights, the matching, and the outcome. For some methods, analytic formulas for SEs have been developed, but for others we have to rely on approximations or computationally intense methods. When using g-computation, the treatment effect does not correspond to a coefficient in the model, so we also need to take into account how SE of the treatment effect is derived from the model coefficient SEs.\nThere are two broad methods we can use for computing CIs and SEs after propensity score analysis: (cluster-) robust SEs and bootstrap CIs, which I described below. See Table 7.1 for a summary of which method should be used in different circumstances.\n\n7.2.1 Robust and cluster-robust SEs\nAlso known as sandwich SEs (due to the form of the formula for computing them), heteroscedasticity-consistent SEs, or Huber-White SEs, robust SEs are an adjustment to the usual maximum likelihood or ordinary least squares SEs that are robust to violations of some of the assumptions required for usual SEs to be valid (MacKinnon and White 1985). Robust SEs have been shown to be conservative (i.e., yield overly large SEs and wide confidence intervals) for estimating the ATE after some forms of weighting (Robins, Hernán, and Brumback 2000), though they can be either conservative or not for other weighting methods and estimands, such as for the ATT (Reifeis and Hudgens 2020) or for entropy balancing (Chan, Yam, and Zhang 2016). Robust SEs treat the estimated weights as if they were fixed and known, ignoring uncertainty in their estimation. Although they are quick and simple to estimate, they should be used with caution, and the bootstrap (described below) should be preferred in most cases.\nA version of robust SEs known as cluster-robust SEs (Liang and Zeger 1986) can be used to account for dependence between observations within clusters (e.g., matched pairs). Abadie and Spiess (2020) demonstrate analytically that cluster-robust SEs are generally valid after matching, whereas regular robust SEs can over- or under-estimate the true sampling variability of the effect estimator depending on the specification of the outcome model (if any) and degree of effect modification. A plethora of simulation studies have further confirmed the validity of cluster-robust SEs after matching (e.g., Austin and Small 2014; Austin 2009; Gayat et al. 2012; Wan 2019).\nTo compute robust SEs after g-computation, a method known as the delta method is used; this is a way to compute the SEs of the derived quantities (the expected potential outcomes and their contrast) from the variance of the coefficients of the outcome models. For nonlinear models (e.g., logistic regression), the delta method is only an approximation subject to error (though in many cases this error is small and shrinks in large samples). Because the delta method relies on the variance of the coefficients from the outcome model, it is important to correctly estimate these variances.\n\n\n7.2.2 Bootstrapping\nBootstrapping is a technique used to simulate the sampling distribution of an estimator by repeatedly drawing samples with replacement and estimating the effect in each bootstrap sample (Efron and Tibshirani 1986; Carpenter and Bithell 2000). From the bootstrap distribution, SEs and CIs can be computed in several ways, including using the standard deviation of the bootstrap estimates as the SE estimate or using the 2.5 and 97.5 percentiles as 95% CI bounds. Although Abadie and Imbens (2008) found analytically that the bootstrap is inappropriate for matching, simulation evidence has found it to be adequate in many cases (Hill and Reiter 2006; Austin and Small 2014; Austin and Stuart 2017).\nTraditionally, bootstrapping involves performing the entire estimation process in each bootstrap sample, including propensity score estimation, matching or weighting, and effect estimation. For weighting, the traditional bootstrap works very well and is generally the best method to use for estimating CIs (Chan, Yam, and Zhang 2016; Austin 2022). For matching, this method may be conservative for matching in some cases (i.e., they are wider than necessary to achieve nominal coverage) (Austin and Small 2014). More accurate intervals have been found when using the matched/cluster bootstrap described by Austin and Small (2014) and Abadie and Spiess (2020). The cluster bootstrap involves sampling matched pairs/strata of units from the matched sample (i.e., after the matching is already done) and estimating the effect within each sample composed of the sampled pairs.\n\n\nTable 7.1: Conditioning methods and corresponding recommended methods for estimating SEs and CIs\n\n\n\n\n\n\n\nMethod\nUse for SEs/CIs\nNotes\n\n\n\n\nPair matching w/o replacement and full matching\nCluster bootstrap or cluster-robust SEs\n\n\n\nPair matching w/ replacement\nRobust SEs\nTraditional bootstrap performs well in simulations, but analytically is invalid; some analytic methods exist for special cases\n\n\nOther matching methods (including propensity score subclassification)\nBootstrap or robust SEs\n\n\n\nPropensity score weighting for the ATE or ATO\nBootstrap or robust SEs\nRobust SEs are conservative\n\n\nPropensity score weighting for the ATT\nBootstrap\nRobust SEs can be conservative or anti-conservative\n\n\n\n\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2008. “On the Failure of the Bootstrap for Matching Estimators.” Econometrica 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\nAbadie, Alberto, and Jann Spiess. 2020. “Robust Post-Matching Inference.” Journal of the American Statistical Association 0 (ja): 1–37. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAustin, Peter C. 2009. “Type I Error Rates, Coverage of Confidence Intervals, and Variance Estimation in Propensity-Score Matched Analyses.” The International Journal of Biostatistics 5 (1). https://doi.org/10.2202/1557-4679.1146.\n\n\n———. 2022. “Bootstrap Vs Asymptotic Variance Estimation When Using Propensity Score Weighting with Continuous and Binary Outcomes.” Statistics in Medicine 41 (22): 4426–43. https://doi.org/10.1002/sim.9519.\n\n\nAustin, Peter C., and Dylan S. Small. 2014. “The Use of Bootstrapping When Using Propensity-Score Matching Without Replacement: A Simulation Study.” Statistics in Medicine 33 (24): 4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2017. “Estimating the Effect of Treatment on Binary Outcomes Using Full Matching on the Propensity Score.” Statistical Methods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nCafri, Guy, and Peter C. Austin. 2023. “Variance Estimation of the Risk Difference When Using Propensity-Score Matching and Weighting with Time-to-Event Outcomes.” Pharmaceutical Statistics, May, pst.2317. https://doi.org/10.1002/pst.2317.\n\n\nCarpenter, James, and John Bithell. 2000. “Bootstrap Confidence Intervals: When, Which, What? A Practical Guide for Medical Statisticians.” Statistics in Medicine 19 (9): 1141–64. https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9&lt;1141::AID-SIM479&gt;3.0.CO;2-F.\n\n\nChan, Kwun Chuen Gary, Sheung Chi Phillip Yam, and Zheng Zhang. 2016. “Globally Efficient Non-Parametric Inference of Average Treatment Effects by Empirical Balancing Calibration Weighting.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 (3): 673–700. https://doi.org/10.1111/rssb.12129.\n\n\nDaniel, Rhian M. 2018. “Double Robustness.” In, 1–14. American Cancer Society. https://doi.org/10.1002/9781118445112.stat08068.\n\n\nEfron, B., and R. Tibshirani. 1986. “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.” Statistical Science 1 (1): 54–75. https://www.jstor.org/stable/2245500.\n\n\nGayat, Etienne, Matthieu Resche-Rigon, Jean-Yves Mary, and Raphaël Porcher. 2012. “Propensity Score Applied to Survival Data Analysis Through Proportional Hazards Models: A Monte Carlo Study.” Pharmaceutical Statistics 11 (3): 222–29. https://doi.org/10.1002/pst.537.\n\n\nHernán, Miguel A. 2010. “The Hazards of Hazard Ratios.” Epidemiology (Cambridge, Mass.) 21 (1): 13–15. https://doi.org/10.1097/EDE.0b013e3181c1ea43.\n\n\nHill, Jennifer, and Jerome P. Reiter. 2006. “Interval Estimation for Treatment Effects Using Propensity Score Matching.” Statistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nKang, Joseph D. Y., and Joseph L. Schafer. 2007. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science 22 (4): 523–39. https://doi.org/10.1214/07-STS227.\n\n\nLiang, Kung-Yee, and Scott L. Zeger. 1986. “Longitudinal Data Analysis Using Generalized Linear Models.” Biometrika 73 (1): 13–22. https://doi.org/10.1093/biomet/73.1.13.\n\n\nMacKinnon, James G, and Halbert White. 1985. “Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” Journal of Econometrics 29 (3): 305–25. https://doi.org/10.1016/0304-4076(85)90158-7.\n\n\nPak, Kyongsun, Hajime Uno, Dae Hyun Kim, Lu Tian, Robert C. Kane, Masahiro Takeuchi, Haoda Fu, Brian Claggett, and Lee-Jen Wei. 2017. “Interpretability of Cancer Clinical Trial Results Using Restricted Mean Survival Time as an Alternative to the Hazard Ratio.” JAMA Oncology 3 (12): 1692. https://doi.org/10.1001/jamaoncol.2017.2797.\n\n\nReifeis, Sarah A., and Michael G. Hudgens. 2020. “On Variance of the Treatment Effect in the Treated Using Inverse Probability Weighting.” arXiv:2011.11874 [Stat], November. http://arxiv.org/abs/2011.11874.\n\n\nRobins, James M., Miguel Ángel Hernán, and Babette Brumback. 2000. “Marginal Structural Models and Causal Inference in Epidemiology.” Epidemiology 11 (5): 550–60. https://doi.org/10.1097/00001648-200009000-00011.\n\n\nSnowden, Jonathan M., Sherri Rose, and Kathleen M. Mortimer. 2011. “Implementation of G-Computation on a Simulated Data Set: Demonstration of a Causal Inference Technique.” American Journal of Epidemiology 173 (7): 731–38. https://doi.org/10.1093/aje/kwq472.\n\n\nStensrud, Mats J., and Miguel A. Hernán. 2020. “Why Test for Proportional Hazards?” JAMA 323 (14): 1401–2. https://doi.org/10.1001/jama.2020.1267.\n\n\nVansteelandt, Stijn, and Niels Keiding. 2011. “Invited Commentary: G-Computationlost in Translation?” American Journal of Epidemiology 173 (7): 739–42. https://doi.org/10.1093/aje/kwq474.\n\n\nWan, Fei. 2019. “Matched or Unmatched Analyses with Propensity-Scorematched Data?” Statistics in Medicine 38 (2): 289–300. https://doi.org/10.1002/sim.7976."
  },
  {
    "objectID": "estimating.html#footnotes",
    "href": "estimating.html#footnotes",
    "title": "7  Estimating the treatment effect",
    "section": "",
    "text": "You may also hear about the potential for “double-robustness” (Daniel 2018; Kang and Schafer 2007), i.e., that the estimate is consistent if either the propensity score model or outcome model are correct, giving you two chances to get it right. I won’t discuss this perspective because the perspective I take is that the conditioning should do all the work of balancing the covariates and removing bias, and the purpose of the outcome model is primarily to increase precision. Truly doubly-robust estimators involve different procedures, different modes of inference, and different priorities in assessment than traditional propensity score analysis.↩︎"
  },
  {
    "objectID": "sensitivity.html",
    "href": "sensitivity.html",
    "title": "8  Sensitivity analysis",
    "section": "",
    "text": "Sensitivity analysis broadly refers to assessing the sensitivity of results to violations of some of the assumptions made. In propensity score analysis, it usually refers to sensitivity to the assumption of satisfaction of the backdoor criterion, i.e., no unmeasured confounding. There is no simple way to test whether observed results are confounded or not; instead, we assess how much our estimates would change for different degrees of hypothetical confounding and, in particular, how strong confounding would have to be to change our inferences (i.e., on the presence or direction of the effect). Sensitivity analysis is an area of ongoing research; presently, there is no single best method to use or one that has achieved widespread adoption.\nD’Agostino McGowan (2022) provide a review of sensitivity analysis methods along with a description of software that implements many of them. Perhaps the most common method of assessing sensitivity to unmeasured confounding is a single-number summary called the “E-value” (VanderWeele and Ding 2017; Haneuse, VanderWeele, and Arterburn 2019), though it is also one of the most controversial and misinterpreted (Ioannidis, Tan, and Blum 2019; VanderWeele, Mathur, and Ding 2019). The E-value is a transformation of the effect estimate that corresponds to the minimum strength of association (on the risk ratio scale) that a hypothetical unmeasured confounder would have to have with the treatment and outcome to fully explain away an observed effect estimate.\nBecause there are no agreed-upon standards for reporting sensitivity to unmeasured confounding and the methods that exist either depend on the analysis method used or are on a particular (non-universal) scale, we cannot recommend a specific approach and instead recommend researchers attend carefully to developments in this area and follow the norms of their respective fields.\n\n\n\n\nD’Agostino McGowan, Lucy. 2022. “Sensitivity Analyses for Unmeasured Confounders.” Current Epidemiology Reports 9 (4): 361–75. https://doi.org/10.1007/s40471-022-00308-6.\n\n\nHaneuse, Sebastien, Tyler J. VanderWeele, and David Arterburn. 2019. “Using the e-Value to Assess the Potential Effect of Unmeasured Confounding in Observational Studies.” JAMA 321 (6): 602–3. https://doi.org/10.1001/jama.2018.21554.\n\n\nIoannidis, John P. A., Yuan Jin Tan, and Manuel R. Blum. 2019. “Limitations and Misinterpretations of e-Values for Sensitivity Analyses of Observational Studies.” Annals of Internal Medicine 170 (2): 108–11. https://doi.org/10.7326/M18-2159.\n\n\nVanderWeele, Tyler J., and Peng Ding. 2017. “Sensitivity Analysis in Observational Research: Introducing the E-Value.” Annals of Internal Medicine 167 (4): 268. https://doi.org/10.7326/M16-2607.\n\n\nVanderWeele, Tyler J., Maya B. Mathur, and Peng Ding. 2019. “Correcting Misinterpretations of the e-Value.” Annals of Internal Medicine 170 (2): 131–32. https://doi.org/10.7326/M18-3112."
  },
  {
    "objectID": "advanced.html#subgroup-analysis",
    "href": "advanced.html#subgroup-analysis",
    "title": "9  Advanced topics",
    "section": "9.1 Subgroup analysis",
    "text": "9.1 Subgroup analysis\nSubgroup analysis is required to understand how treatments affect different types of patients and to be able to provide reasoned recommendations when information about patients is available (in contrast to the broad policy-based recommendations implied by the usual estimands). Subgroup analysis can be done simply by performing separate analyses within each subgroup (Green and Stuart 2014), though in some case it can be beneficial to share information (e.g., estimation of the propensity score or outcome model) across subgroups (Dong et al. 2020).\nIt is also important to remember that performing subgroup analysis does not allow one to make a causal claim about the effect of subgroup membership on the treatment effect unless additional work is done to remove confounding from subgroup membership. For example, one may be interested in a subgroup analysis stratified by hospital. It may be that the treatment effect in one hospital differs from that in another, but that does not mean which hospital one goes to causes differences in the treatment effect (e.g., because of different quality of care); it may simply be that one hospital caters to patients for whom treatment is less effective (e.g., because of systemic issues that cause people both to suffer from comorbidities that change the treatment effect and to live closer to one hospital than another). This distinction between a scenario in which subgroup membership causes treatment effect heterogeneity and one in which subgroup membership is merely associated with treatment effect heterogeneity is described in detail in (VanderWeele 2009)."
  },
  {
    "objectID": "advanced.html#multi-category-and-continuous-treatments",
    "href": "advanced.html#multi-category-and-continuous-treatments",
    "title": "9  Advanced topics",
    "section": "9.2 Multi-category and continuous treatments",
    "text": "9.2 Multi-category and continuous treatments\nTreatments do not have to be binary to be used with propensity score analysis. Methods also exist for multi-category and continuous treatments. An example of a multi-category treatment might be drug type in a study comparing two drugs to each other and to control. (Strasser et al. 2022) considered virus variant a multi-category exposure when examining the effect of COVID subvariant (Delta, Omicron, and Omicron BA.2) on patient health outcomes. An example of a continuous treatment might be the effect of pollutant exposure on mortality, as examined by (Wu et al. 2022).\n\n9.2.1 Multi-category treatments\nEstimating effects for multi-category treatment involves adjusting the sample so that the distributions of covariates in all categories resemble each other and some target population corresponding to the estimand of interest. This can be done using matching (Lopez and Gutman 2017) or weighting (McCaffrey et al. 2013). Instead of a single-valued propensity score, each unit has a vector-valued “generalized” propensity score corresponding to the probability of receiving each level of treatment (Imbens 2000). For example, for a three-level treatment, an individual unit may have a generalized propensity score of \\([.1, .4, .5]\\). McCaffrey et al. (2013) and Li and Li (2019) describe how to use these generalized propensity scores to compute weights. Currently, weighting methods are better developed and easier to use than matching methods for multi-category treatments and are available in WeightIt.\n\n\n9.2.2 Continuous treatments\nThe usual estimand for a continuous treatment is the average dose-response function (ADRF), which links the expected potential outcome (i.e., the average outcome if everyone was assigned to a single treatment value) to the corresponding treatment level. Propensity score analysis for continuous treatments involves adjusting the sample so that the treatment is independent from the covariates. This can be done using matching (Wu et al. 2022) or weighting (Robins, Hernán, and Brumback 2000; Zhu, Coffman, and Ghosh 2015; Huling, Greifer, and Chen 2023) (available in WeightIt). The propensity score is instead represented as a single-valued generalized propensity score corresponding to the conditional density of treatment given the covariates (i.e., rather than the probability, which would be 0 for a all values of a truly continuous treatment) (Hirano and Imbens 2005). Balance is often assessed using the correlations between the treatment and each covariate in the adjusted sample (Austin 2019), though more holistic measures such as the distance covariance have also been developed (Huling, Greifer, and Chen 2023). To estimate the ADRF, one can fit a flexible model for the outcome given the treatment in the weighted sample."
  },
  {
    "objectID": "advanced.html#longitudinalsequential-treatments",
    "href": "advanced.html#longitudinalsequential-treatments",
    "title": "9  Advanced topics",
    "section": "9.3 Longitudinal/sequential treatments",
    "text": "9.3 Longitudinal/sequential treatments\nMethods have been developed for estimating the effect of a treatment that can occur at multiple time points. For example, Robins, Hernán, and Brumback (2000) described methods for estimating the effect of zidovudine (AZT) treatment on mortality in HIV-infected patients, where treatment was defined each day since start of follow-up as the those dose of AZT received that day. These special methods must be used when confounding is time-varying, i.e., confounders of subsequent treatment and the outcome are themselves affected by previous treatments. Simply adjusting for these time-varying confounders by regression adjustment or standard propensity score analysis causes the same problems that adjusting for any post-treatment variable does.\nThe methods used for adjusting for time-varying confounding are called “g-methods” and are described in Hernán and Robins (2020). The simplest one is inverse probability weighting of marginal structural models, which essentially involves creating a propensity score weight at each time point and multiplying them together (available in WeightIt); ideally, this yields a scenario analogous to one in which treatment is randomized at each time point. Thoemmes and Ong (2016) and Robins, Hernán, and Brumback (2000) provide clear examples of the method."
  },
  {
    "objectID": "advanced.html#missing-data",
    "href": "advanced.html#missing-data",
    "title": "9  Advanced topics",
    "section": "9.4 Missing data",
    "text": "9.4 Missing data\nMissing data is often present in the analysis of real datasets. There are a variety of reasons why data could be missing: an administrative error, loss to follow-up, or patient refusal to provide information are some examples. Handling missing data generally is a serious topic that requires expertise to do correctly, though there are mainstream methods that are commonly used and have been shown to be compatible with propensity score analysis and can yield accurate results if certain assumption about why the data are missing are met (Cham and West 2016). The most common methods for dealing with missing data in propensity score analysis are multiple imputation (Rubin 2004) and censoring weights (Hernán and Robins 2020, Ch 12.6).\nImputation involves making a guess about the true value of each missing value. This guess often comes from a predictive model that describes the relationships among variables in the data. Instead of making a single guess, multiple imputation involves making many guesses, each stored in a separate version of the dataset with the guesses filled in. The analysis occurs in each imputed dataset, and then the results are pooled across datasets to arrive at a final single estimate. Although there have been doubts about the best way to perform propensity score analysis with multiply imputed data, simulations frequently verify that the standard approach described above yields the most accurate results (Leyrat et al. 2019). The MatchThem package provides some utilities for matching and weighting with multiple imputed data (Pishgar et al. 2021), and cobalt supports assessing balance across imputations (Greifer 2020).\nCensoring weights are an alternative to imputation that are more commonly used when a single variable, e.g., the outcome, is missing for some units. Censoring weights discard any units with missing data and weight the remaining units to resemble the full sample (i.e., the original sample that included those with missing data) (Hernán and Robins 2020, Ch 12.6). Censoring weights are multiplied by propensity score weights when both are used to create a final set of weights that adjust for both confounding and censoring. Censoring weights are especially common with longitudinal treatments and in survival analysis.\n\n\n\n\nAustin, Peter C. 2019. “Assessing Covariate Balance When Using the Generalized Propensity Score with Quantitative or Continuous Exposures.” Statistical Methods in Medical Research 28 (5): 1365–77. https://doi.org/10.1177/0962280218756159.\n\n\nCham, Heining, and Stephen G. West. 2016. “Propensity Score Analysis with Missing Data.” Psychological Methods 21 (3): 427–45. https://doi.org/10.1037/met0000076.\n\n\nDong, Jing, Junni L Zhang, Shuxi Zeng, and Fan Li. 2020. “Subgroup Balancing Propensity Score.” Statistical Methods in Medical Research 29 (3): 659–76. https://doi.org/10.1177/0962280219870836.\n\n\nGreen, Kerry M., and Elizabeth A. Stuart. 2014. “Examining Moderation Analyses in Propensity Score Methods: Application to Depression and Substance Use.” Journal of Consulting and Clinical Psychology, Advances in data analytic methods, 82 (5): 773–83. https://doi.org/10.1037/a0036515.\n\n\nGreifer, Noah. 2020. Cobalt: Covariate Balance Tables and Plots. https://CRAN.R-project.org/package=cobalt.\n\n\nHernán, Miguel A, and James M Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC. https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf.\n\n\nHirano, Keisuke, and Guido W. Imbens. 2005. “The Propensity Score with Continuous Treatments.” In, edited by Andrew Gelman and Xiao-Li Meng, 73–84. Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/0470090456.ch7.\n\n\nHuling, Jared D., Noah Greifer, and Guanhua Chen. 2023. “Independence Weights for Causal Inference with Continuous Treatments.” Journal of the American Statistical Association 0 (ja): 1–25. https://doi.org/10.1080/01621459.2023.2213485.\n\n\nImbens, Guido W. 2000. “The Role of the Propensity Score in Estimating Dose-Response Functions.” Biometrika 87 (3): 706–10. https://www.jstor.org/stable/2673642.\n\n\nLeyrat, Clémence, Shaun R Seaman, Ian R White, Ian Douglas, Liam Smeeth, Joseph Kim, Matthieu Resche-Rigon, James R Carpenter, and Elizabeth J Williamson. 2019. “Propensity Score Analysis with Partially Observed Covariates: How Should Multiple Imputation Be Used?” Statistical Methods in Medical Research 28 (1): 3–19. https://doi.org/10.1177/0962280217713032.\n\n\nLi, Fan, and Fan Li. 2019. “Propensity Score Weighting for Causal Inference with Multiple Treatments.” The Annals of Applied Statistics 13 (4): 2389–2415. https://doi.org/10.1214/19-AOAS1282.\n\n\nLopez, Michael J., and Roee Gutman. 2017. “Estimation of Causal Effects with Multiple Treatments: A Review and New Ideas.” Statistical Science 32 (3): 432–54. https://doi.org/10.1214/17-STS612.\n\n\nMcCaffrey, Daniel F., Beth Ann Griffin, Daniel Almirall, Mary Ellen Slaughter, Rajeev Ramchand, and Lane F. Burgette. 2013. “A Tutorial on Propensity Score Estimation for Multiple Treatments Using Generalized Boosted Models.” Statistics in Medicine 32 (19): 3388–3414. https://doi.org/10.1002/sim.5753.\n\n\nPishgar, Farhad, Noah Greifer, Clémence Leyrat, and Elizabeth Stuart. 2021. “MatchThem:: Matching and Weighting After Multiple Imputation.” The R Journal 13 (2): 292305. https://doi.org/10.32614/RJ-2021-073.\n\n\nRobins, James M., Miguel Ángel Hernán, and Babette Brumback. 2000. “Marginal Structural Models and Causal Inference in Epidemiology.” Epidemiology 11 (5): 550–60. https://doi.org/10.1097/00001648-200009000-00011.\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in Surveys. Wiley Classics Library. Hoboken, N.J: Wiley-Interscience.\n\n\nStrasser, Zachary H., Noah Greifer, Aboozar Hadavand, Shawn N. Murphy, and Hossein Estiri. 2022. “Estimates of SARS-CoV-2 Omicron BA.2 Subvariant Severity in New England.” JAMA Network Open 5 (10): e2238354. https://doi.org/10.1001/jamanetworkopen.2022.38354.\n\n\nThoemmes, Felix J., and Anthony D. Ong. 2016. “A Primer on Inverse Probability of Treatment Weighting and Marginal Structural Models.” Emerging Adulthood 4 (1): 40–59. https://doi.org/10.1177/2167696815621645.\n\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology 20 (6): 863–71. https://www.jstor.org/stable/25662776.\n\n\nWu, Xiao, Fabrizia Mealli, Marianthi-Anna Kioumourtzoglou, Francesca Dominici, and Danielle Braun. 2022. “Matching on Generalized Propensity Scores with Continuous Exposures.” Journal of the American Statistical Association 0 (0): 1–29. https://doi.org/10.1080/01621459.2022.2144737.\n\n\nZhu, Yeying, Donna L. Coffman, and Debashis Ghosh. 2015. “A Boosting Algorithm for Estimating Generalized Propensity Scores with Continuous Treatments.” Journal of Causal Inference 3 (1). https://doi.org/10.1515/jci-2014-0022."
  },
  {
    "objectID": "reporting.html#reporting-the-balancing-method-and-results",
    "href": "reporting.html#reporting-the-balancing-method-and-results",
    "title": "10  Reporting the Analysis",
    "section": "10.1 Reporting the balancing method and results",
    "text": "10.1 Reporting the balancing method and results\nThe balancing method is central to the validity of the analysis, so its nature and performance must be reported for readers to be able to correctly interpret the results. Below are some of the qualities that should be reported:\n\nThe conditioning method used (e.g., matching, weighting, or subclassification) and the specifics of the method. For example, for matching, was optimal or nearest neighbor matching used? Was matching done with or without replacement? Were calipers or exact matching constraints applied? What was the distance measure used to define close matches? For weighting, was propensity score weighting or entropy balancing used? Were the weights trimmed?\nHow the propensity score, if any, was estimated. For example, was logistic regression used? Were any polynomial terms, interactions, or covariate transformations included in the model? Was generalized boosted modeling (GBM) used? How were the tuning parameters selected (e.g., optimizing cross-validation accuracy or a multivariate measure of balance)?\nThe intended estimand and the estimand that resulted from the analysis. For example, perhaps the ATT was targeted using nearest neighbor matching, but the only way to achieve balance was with a caliper, which changes the estimand to the ATO. Or perhaps there was not enough overlap to estimate the ATE using entropy balancing so ATO weights were used instead. Ideally this also includes a rationale for the choice of estimand, which should be reflected in the body of the paper (e.g., if the paper implies a universal treatment policy or a policy applied only to a certain group of patients).\nThe covariate balance results of the conditioning procedure. Balance is often reported using a table or plot (e.g., a love/dot plot) that contains the balance statistics (e.g., SMD and KS statistic) for each covariate. Balance should also be summarized in a way that provides a more complete story than the univariate statistics computed on the original variables can tell, e.g., by mentioning the worst balance for all squares, cubes and fourth powers of the covariates and all two-way interactions (individual balance statistics on these do not need to be reported, but demonstrating that balance is achieved on these aspects of the covariate distribution lends more credibility to the results). For example, entropy balancing guarantees SMDs of zero on all covariate and requested transformations thereof; simply mentioning this fact and that entropy balancing was successful provides just as much information as presenting SMDs for each covariate individually.\nThe effective sample size of the adjusted sample. This should be reported for each treatment group separately. A nonsignificant result found in a sample with a low ESS would be interpreted exactly as it would be in any under-powered study, and it is important that readers understand the sample size context when judging the results of a study. The raw sample size does not provide this information; for example, weighting does not change the raw sample size but can dramatically reduce the effective sample size, and readers must be aware of that.\n\nSome of these aspects might be unfamiliar to a reader, and so it is useful to include a short description of them, especially if they are a newer method or would be confusing without proper context. For example, a sentence describing the use of entropy balancing might go as follows:\n\nTo adjust for confounding by measured confounders, we used entropy balancing (Hainmueller 2012), a version of propensity score weighting that guarantees exact balance on the covariate means while minimizing the variability of the weights without explicitly modeling a propensity score.\n\nA footnote describing the ESS might go as follows:\n\nThe effective sample size in an estimate of the size of a hypothetical unweighted sample that carries the same precision of our weighted sample and reflects the loss in precision due to weighting, analogous to discarding units when matching.\n\nOf course, more detail on the methods is always better, and word count restrictions should not be an excuse for incompletely reporting the most critical part of a study’s methodology."
  },
  {
    "objectID": "reporting.html#reporting-the-effect-estimation-procedure",
    "href": "reporting.html#reporting-the-effect-estimation-procedure",
    "title": "10  Reporting the Analysis",
    "section": "10.2 Reporting the effect estimation procedure",
    "text": "10.2 Reporting the effect estimation procedure\nIt’s also important to report the effect estimation procedure, as how the effect is estimated can affect its interpretation. The following elements should be reported:\n\nThe outcome model used. Was it a logistic, Cox, or Poisson regression model? Were covariates included? If so, how were covariates included (e.g., as main effects or fully interacted with treatment)? How were covariates selected to be in the model (e.g., all were included, only those thought to explain the most variability in the outcome, only those with some remaining imbalance, etc.)? One must also clarify how the weights were used in the model, i.e., by using a weighted regression.\nThe method of estimating the treatment effect from the model. Although in this document we have recommended g-computation, in some cases simply using the coefficient on treatment in the outcome model is sufficient for estimating the treatment effect (i.e., with linear outcome models or with models that lack any covariates). As previously mentioned, it is critical that the coefficient on treatment not be used as an effect estimate when covariates are included in the outcome model and the estimand is a marginal effect.\nThe method of computing the SEs and CIs. Was it by the delta method? Was a robust or cluster-robust standard error used? Was bootstrapping used? Was it the traditional bootstrap? How were CIs extracted from the bootstrap procedure (e.g., using percentiles or bias-correct and accelerated CIs)? Remember that under no circumstances should the maximum likelihood estimates of the SEs be used for inference; matching and weighting require special treatment of the estimates for them to be valid."
  },
  {
    "objectID": "reporting.html#reporting-the-effect-estimate",
    "href": "reporting.html#reporting-the-effect-estimate",
    "title": "10  Reporting the Analysis",
    "section": "10.3 Reporting the effect estimate",
    "text": "10.3 Reporting the effect estimate\nOf course, one needs to report the effect estimate itself. Ideally the estimate and its CI are on a natural, interpretable scale (e.g., the risk ratio rather than the log odds ratio), though it may also be useful to include the scale on which inference is performed. For example, one may have computed the log risk ratio in order to compute its standard error and p-value for the test that it is equal to 0 (i.e., that the risk ratio is equal to 1), which can be reported, but the critical clinically useful measure is the risk ratio itself and its CI."
  },
  {
    "objectID": "reporting.html#reporting-software",
    "href": "reporting.html#reporting-software",
    "title": "10  Reporting the Analysis",
    "section": "10.4 Reporting software",
    "text": "10.4 Reporting software\nIn order to ensure results are replicable, one must report the specific software used and the version of that software. For example, one must specify that they are running the analysis in R (including the specific version of R used) and must name and cite all R packages used in the analysis (and their version). Instructions for citing a specific R package can be found by running citation(\"pkg_name\"), e.g., citation(\"MatchIt\"). In some cases, using a specific function in an R package has additional citation requirements. For example, when using optimal pair matching in MatchIt, in addition to citing MatchIt, one must also cite optmatch, which MatchIt uses under the hood. This is explained the documentation for using optimal pair matching with MatchIt, which can be accessed using ?method_optimal.\nOften, the packages cited will include MatchIt or WeightIt for performing the matching or weighting, any package mentioned in the documentation page for the specific method used, cobalt for assessing balance, and marginaleffects for estimating the treatment effect. For survival outcomes, the survival package might be used. Citing packages in critical for ensuring the work done by package authors is correctly attributed. It also ensures someone attempting to replicate one’s work can do so without having to infer the software used for the analysis."
  },
  {
    "objectID": "reporting.html#reporting-limitations",
    "href": "reporting.html#reporting-limitations",
    "title": "10  Reporting the Analysis",
    "section": "10.5 Reporting limitations",
    "text": "10.5 Reporting limitations\nIn order to provide context for the estimate and prevent misinterpretations, it is important to report the limitations of the study. Limitations often come in the form of assumptions that cannot be verified that would change the interpretation of the results if false. For example, if it is impossible to verify that all confounders of the treatment and outcome have been collected and adjusted for, the estimate cannot be interpreted as causal and may be biased for the true causal effect, in which case the inability to make a definitive causal claim is a limitation. If the most useful or desired estimand was the ATE, but aspects of the sample and analysis required that the ATO be estimated instead to preserve precision or achieve balance, the inability to generalize the effect to the intended population would be a limitation.\nThese are in addition to the limitations you would report even in a clinical trial, e.g., with respect to measurement error in the treatment, covariates, or outcome, timing of the outcome, treatment compliance, missing data, etc. The limitations section of a paper does not have to be long but anticipating criticism of the paper by acknowledging its limitations can go a long way in getting it accepted by reviewers.\n\n\n\n\nArguelles, Gabriel R., Max Shin, Drake G. Lebrun, Christopher J. DeFrancesco, Peter D. Fabricant, and Keith D. Baldwin. 2022. “A Systematic Review of Propensity Score Matching in the Orthopedic Literature.” HSS Journal, April, 15563316221082632. https://doi.org/10.1177/15563316221082632.\n\n\nHainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nZakrison, T. L., Peter C. Austin, and V. A. McCredie. 2018. “A Systematic Review of Propensity Score Methods in the Acute Care Surgery Literature: Avoiding the Pitfalls and Proposing a Set of Reporting Guidelines.” European Journal of Trauma and Emergency Surgery 44 (3): 385–95. https://doi.org/10.1007/s00068-017-0786-6."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "11  Example Data",
    "section": "",
    "text": "Below, we’ll demonstrate how to perform matching and weighting in R. We’ll use the famous right-heart catheterization (RHC) dataset analyzed in Connors et al. (1996), which examines the effect of RHC on death by 60 days. Connors et al. (1996) used 1:1 matching with a caliper to estimate the effect, which corresponds to an ATO (though they provided no justification for this choice of estimand). It turns out this matters quite a bit; the ATT, ATC, and ATE differ from each other and lead to different conclusions about the risk of RHC.\nThe choice of estimand depends on the policy implied by the analysis. Are we interested in examining whether RHC is harmful and should be withheld from patients receiving it? If so, we are interested in the ATT of RHC. Are we interested in examining whether RHC would benefit patients not receiving it? If so, we are interested in the ATC of RHC. Are we interested in the average effect of RHC for the whole study population? If so, we are interested in the ATE of RHC.\nWe’ll assume that if we are making a causal inference about the effect of RHC, we have collected a sufficient set of variables to remove confounding. This may be a long list, but to keep the example short, we’ll use a list of 13 covariates thought to be related to receipt of RHC and death at 60 days, all measured prior to receipt of RHC.\nLet’s take a look at our dataset:\n\nsummary(rhc)\n\n      aps1           meanbp1           pafi1           crea1         \n Min.   :  3.00   Min.   :  0.00   Min.   : 11.6   Min.   : 0.09999  \n 1st Qu.: 41.00   1st Qu.: 50.00   1st Qu.:133.3   1st Qu.: 1.00000  \n Median : 54.00   Median : 63.00   Median :202.5   Median : 1.50000  \n Mean   : 54.67   Mean   : 78.52   Mean   :222.3   Mean   : 2.13302  \n 3rd Qu.: 67.00   3rd Qu.:115.00   3rd Qu.:316.6   3rd Qu.: 2.39990  \n Max.   :147.00   Max.   :259.00   Max.   :937.5   Max.   :25.09766  \n     hema1           paco21          surv2md1          resp1         card     \n Min.   : 2.00   Min.   :  1.00   Min.   :0.0000   Min.   :  0.00   No :3804  \n 1st Qu.:26.10   1st Qu.: 31.00   1st Qu.:0.4709   1st Qu.: 14.00   Yes:1931  \n Median :30.00   Median : 37.00   Median :0.6280   Median : 30.00             \n Mean   :31.87   Mean   : 38.75   Mean   :0.5925   Mean   : 28.09             \n 3rd Qu.:36.30   3rd Qu.: 42.00   3rd Qu.:0.7430   3rd Qu.: 38.00             \n Max.   :66.19   Max.   :156.00   Max.   :0.9620   Max.   :100.00             \n      edu             age            race          sex            RHC        \n Min.   : 0.00   Min.   : 18.04   white:4460   Female:2543   Min.   :0.0000  \n 1st Qu.:10.00   1st Qu.: 50.15   black: 920   Male  :3192   1st Qu.:0.0000  \n Median :12.00   Median : 64.05   other: 355                 Median :0.0000  \n Mean   :11.68   Mean   : 61.38                              Mean   :0.3808  \n 3rd Qu.:13.00   3rd Qu.: 73.93                              3rd Qu.:1.0000  \n Max.   :30.00   Max.   :101.85                              Max.   :1.0000  \n     death      \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :1.000  \n Mean   :0.649  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nOur treatment variable is RHC (1 for receipt, 0 for non-receipt), our outcome is death (1 for died at 60 days, 0 otherwise), and the other variables are covariates thought to remove confounding, which include a mix of continuous and categorical variables.\nLet’s examine balance on the variables between the treatment groups using cobalt, which provides the function bal.tab() for creating a balance table containing balance statistics for each variables.\n\nlibrary(\"cobalt\")\n\n cobalt (Version 4.5.1.9000, Build Date: 2023-06-04)\n\n\nWe’ll request the standardized mean difference by including \"m\" in the stats argument and setting binary = \"std\" (by default binary variables are not standardized) and we’ll request KS statistics by including \"ks\" in stats. Supplying the treatment and covariates in the first argument using a formula and supplying the data set gives us the following:\n\nbal.tab(RHC ~ aps1 + meanbp1 + pafi1 + crea1 + hema1 +\n          paco21 + surv2md1 + resp1 + card + edu +\n          age + race + sex, data = rhc,\n        stats = c(\"m\", \"ks\"), binary = \"std\")\n\nNote: `s.d.denom` not specified; assuming pooled.\n\n\nBalance Measures\n              Type Diff.Un  KS.Un\naps1       Contin.  0.5014 0.2127\nmeanbp1    Contin. -0.4551 0.2117\npafi1      Contin. -0.4332 0.1816\ncrea1      Contin.  0.2696 0.2011\nhema1      Contin. -0.2693 0.1479\npaco21     Contin. -0.2486 0.1081\nsurv2md1   Contin. -0.1985 0.0957\nresp1      Contin. -0.1655 0.0910\ncard_Yes    Binary  0.2950 0.1395\nedu        Contin.  0.0914 0.0511\nage        Contin. -0.0614 0.0703\nrace_white  Binary  0.0152 0.0063\nrace_black  Binary -0.0310 0.0114\nrace_other  Binary  0.0208 0.0050\nsex_Male    Binary  0.0931 0.0462\n\nSample sizes\n    Control Treated\nAll    3551    2184\n\n\nWe can see significant imbalances in many of the covariates, with high SMDs (greater than .1) and KS statistics (greater than .1, but there is no accepted threshold for these). We can also see the sample sizes for each treatment group. Note that because they are somewhat close in size (the control group is not even twice the size of the treatment group), this will limit the available matching options available and might affect our ability to achieve balance using methods that require a large pool of controls relative to the treated group.\nOther balance statistics can be requested, too, using the stats argument. It is straightforward to assess balance on particular transformations of covariates using the addl argument, e.g., addl = ~age:educ to assess balance on the interaction (i.e., product) of age and educ. We can also supply int = TRUE and poly = 3, for example, to assess balance on all pairwise interactions of covariates and all squares and cubes of the continuous covariates. This can make for large tables, but there are ways to keep them short and summarize them. For example, we can hide the balance table and request the number of covariates that fail to satisfy balance criteria and the covariates with the worst imbalance using code below:\n\nbal.tab(RHC ~ aps1 + meanbp1 + pafi1 + crea1 + hema1 +\n          paco21 + surv2md1 + resp1 + card + edu +\n          age + race + sex, data = rhc,\n        int = TRUE, poly = 3,\n        stats = c(\"m\", \"ks\"), binary = \"std\",\n        thresholds = c(m = .1, ks = .1),\n        disp.bal.tab = FALSE)\n\nNote: `s.d.denom` not specified; assuming pooled.\n\n\nBalance tally for mean differences\n                   count\nBalanced, &lt;0.1        61\nNot Balanced, &gt;0.1   105\n\nVariable with the greatest mean difference\n        Variable Diff.Un     M.Threshold.Un\n meanbp1 * pafi1 -0.5965 Not Balanced, &gt;0.1\n\nBalance tally for KS statistics\n                   count\nBalanced, &lt;0.1        80\nNot Balanced, &gt;0.1    86\n\nVariable with the greatest KS statistic\n        Variable  KS.Un    KS.Threshold.Un\n meanbp1 * pafi1 0.2562 Not Balanced, &gt;0.1\n\nSample sizes\n    Control Treated\nAll    3551    2184\n\n\nWe can see that many covariates and their transformations (interactions, squares, and cubes) are not balanced based on our criteria for SMDs or KS statistics. We’ll use matching and weighting in the next sections to attempt to achieve balance on the covariates.\n\n\n\n\nConnors, Alfred F, Neal V Dawson, Frank E Harrell, Douglas Wagner, Norman Desbiens, Lee Goldman, Albert W Wu, et al. 1996. “The Effectiveness of Right Heart Catheterization in the Initial Care of Critically III Patients.” JAMA: The Journal of the American Medical Association 276 (11): 889. https://doi.org/10.1001/jama.1996.03540110043030."
  },
  {
    "objectID": "ex-match.html#footnotes",
    "href": "ex-match.html#footnotes",
    "title": "12  Matching",
    "section": "",
    "text": "Optimal full matching (method = \"full\") tends to work a bit better, but can be much slower for larger datasets.↩︎\nNote that this behavior, and the names of the new columns created, can be customized by the user.↩︎\nIn practice, it is okay to omit the newdata argument, especially when balance is excellent.↩︎"
  },
  {
    "objectID": "ex-weight.html#footnotes",
    "href": "ex-weight.html#footnotes",
    "title": "13  Weighting",
    "section": "",
    "text": "Note that the ATE can be targeted by matching (not 1:1 matching, but other methods) and the ATT and other estimands can be targeted by weighting; don’t think matching is for the ATT and weighting is for the ATE. Use whichever method yields the best performance and would be best understood by your audience.↩︎\nNote, in this case, the conclusions would have been the same regardless of which weighting method we moved forward with.↩︎"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "14  Conclusion",
    "section": "",
    "text": "In this guide, we have described the basic conceptual and practical steps for running a propensity score analysis. Despite its popularity, propensity score analysis remains a poorly understood method that requires much conceptual work prior to the analysis in order for its conclusions to be valid and interpretable. The conceptual issues primarily concern the estimand (the causal quantity being estimated) and the assumptions required for causal inference.\nApplying propensity score methods in practice requires care. Using the default methods in software packages or the most commonly used methods in a given field will not guarantee accurate results. Propensity score analysis requires a process of specification, assessment, and respecification before one can move forward with effect estimation. Assessment involves ensuring the covariate distributions are balanced, ensuring the remaining sample is representative of the target population, and ensuring the size of the remaining sample is sufficient for estimating an effect with precision. The most commonly applied methods make sacrifices in the name of simplicity, but those sacrifices often do not bear encouraging results. Newer methods that are just as easy to use but may be less studied and less familiar to researchers, reviewers, and audiences often perform much better, and should be prioritized over familiar methods for making valid inferences.\nIn particular, we recommend matching methods that relax the restriction of 1:1 matching when possible and retain the target estimand; (optimal or generalized) full matching and subclassification often outperform 1:1 matching without replacement. We recommend weighting methods that bypass estimation of a propensity score and estimate weights that directly balance the covariate distributions, such as entropy balancing and energy balancing. Whenever a propensity score is used, flexible machine learning models that can account for potential nonlinearities in the treatment model should be prioritized over simple logistic regression models. Of course, every method can be assessed empirically on a given dataset; results from simulation studies and broad recommendations can only provide hints of what methods might work best but cannot guarantee that a given method will succeed in a given dataset. It may be that simple methods are adequate for the data at hand; however, it is critical that this scenario be evaluated.\nAlthough much focus in propensity score analysis has been on the method itself, estimating the treatment effect after matching or weighting is just as important and can have big impacts on the interpretation of the results. We recommend never using the coefficient on treatment in an outcome model as a treatment effect estimate unless no covariates are included in the model. That said, we recommend always including covariates when possible to improve precision. G-computation provides a means of computing a treatment effect on any effect measure scale from any outcome model in such a way that covariates can be included while retaining the target estimand. Standard errors and confidence intervals require consideration as well; in general, bootstrapping works well for both matching and weighting, though there are faster analytic methods that can yield comparable performance without requiring such a computationally intensive procedure.\nPropensity score analysis doesn’t automatically give one causality, but it can help along the way. Satisfaction of the assumptions of causal inference are what allow the statistical quantities estimated using propensity score analysis to be interpreted as causal. In additional to these assumptions, purely statistical assumptions are required for an estimate to be accurate. Researchers must articulate these assumption and express any uncertainty around them as limitations that are often inherent to observational research. Whenever possible, researchers should provide information to readers about assessments of the the quality of the method used and about sensitivity to potential violations of these assumptions.\nWe hope this guide has been helpful in allowing researchers to move toward best practices in propensity score analysis with the aim of producing more valid and reproducible research."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadie, Alberto, and Guido W. Imbens. 2006. “Large Sample\nProperties of Matching Estimators for Average Treatment Effects.”\nEconometrica 74 (1): 235–67. https://doi.org/10.1111/j.1468-0262.2006.00655.x.\n\n\n———. 2008. “On the Failure of the Bootstrap for Matching\nEstimators.” Econometrica 76 (6): 1537–57. https://doi.org/10.3982/ECTA6474.\n\n\n———. 2016. “Matching on the Estimated Propensity Score.”\nEconometrica 84 (2): 781–807. https://doi.org/10.3982/ECTA11293.\n\n\nAbadie, Alberto, and Jann Spiess. 2020. “Robust Post-Matching\nInference.” Journal of the American Statistical\nAssociation 0 (ja): 1–37. https://doi.org/10.1080/01621459.2020.1840383.\n\n\nAlam, Shomoita, Erica E. M. Moodie, and David A. Stephens. 2019.\n“Should a Propensity Score Model Be Super? The Utility of Ensemble\nProcedures for Causal Adjustment.” Statistics in\nMedicine 38 (9): 1690–1702. https://doi.org/10.1002/sim.8075.\n\n\nAli, M. Sanni, Rolf H. H. Groenwold, Svetlana V. Belitser, Wiebe R.\nPestman, Arno W. Hoes, Kit C. B. Roes, Anthonius de Boer, and Olaf H.\nKlungel. 2015. “Reporting of Covariate Selection and Balance\nAssessment in Propensity Score Analysis Is Suboptimal: A Systematic\nReview.” Journal of Clinical Epidemiology 68 (2):\n122–31. https://doi.org/10.1016/j.jclinepi.2014.08.011.\n\n\nArguelles, Gabriel R., Max Shin, Drake G. Lebrun, Christopher J.\nDeFrancesco, Peter D. Fabricant, and Keith D. Baldwin. 2022. “A\nSystematic Review of Propensity Score Matching\nin the Orthopedic Literature.” HSS Journal,\nApril, 15563316221082632. https://doi.org/10.1177/15563316221082632.\n\n\nAustin, Peter C. 2009. “Type I Error Rates,\nCoverage of Confidence Intervals, and\nVariance Estimation in Propensity-Score Matched\nAnalyses.” The International Journal of\nBiostatistics 5 (1). https://doi.org/10.2202/1557-4679.1146.\n\n\n———. 2011a. “Optimal Caliper Widths for Propensity-Score Matching\nWhen Estimating Differences in Means and Differences in Proportions in\nObservational Studies.” Pharmaceutical Statistics 10\n(2): 150–61. https://doi.org/10.1002/pst.433.\n\n\n———. 2011b. “An Introduction to Propensity Score Methods for\nReducing the Effects of Confounding in Observational Studies.”\nMultivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\n———. 2014. “A Comparison of 12 Algorithms for Matching on the\nPropensity Score.” Statistics in Medicine 33 (6):\n1057–69. https://doi.org/10.1002/sim.6004.\n\n\n———. 2019. “Assessing Covariate Balance When Using the Generalized\nPropensity Score with Quantitative or Continuous Exposures.”\nStatistical Methods in Medical Research 28 (5): 1365–77. https://doi.org/10.1177/0962280218756159.\n\n\n———. 2022. “Bootstrap Vs Asymptotic Variance Estimation When Using\nPropensity Score Weighting with Continuous and Binary Outcomes.”\nStatistics in Medicine 41 (22): 4426–43. https://doi.org/10.1002/sim.9519.\n\n\nAustin, Peter C., and Dylan S. Small. 2014. “The Use of\nBootstrapping When Using Propensity-Score Matching Without Replacement:\nA Simulation Study.” Statistics in Medicine 33 (24):\n4306–19. https://doi.org/10.1002/sim.6276.\n\n\nAustin, Peter C., and Elizabeth A. Stuart. 2015a. “Optimal Full\nMatching for Survival Outcomes: A Method That Merits More Widespread\nUse.” Statistics in Medicine 34 (30): 3949–67. https://doi.org/10.1002/sim.6602.\n\n\n———. 2015b. “Moving Towards Best Practice When Using Inverse\nProbability of Treatment Weighting (IPTW) Using the Propensity Score to\nEstimate Causal Treatment Effects in Observational Studies.”\nStatistics in Medicine 34 (28): 3661–79. https://doi.org/10.1002/sim.6607.\n\n\n———. 2017a. “The Performance of Inverse Probability of Treatment\nWeighting and Full Matching on the Propensity Score in the Presence of\nModel Misspecification When Estimating the Effect of Treatment on\nSurvival Outcomes.” Statistical Methods in Medical\nResearch 26 (4): 1654–70. https://doi.org/10.1177/0962280215584401.\n\n\n———. 2017b. “Estimating the Effect of Treatment on Binary Outcomes\nUsing Full Matching on the Propensity Score.” Statistical\nMethods in Medical Research 26 (6): 2505–25. https://doi.org/10.1177/0962280215601134.\n\n\nBenedetto, Umberto, Stuart J Head, Gianni D Angelini, and Eugene H\nBlackstone. 2018. “Statistical Primer: Propensity Score Matching\nand Its Alternatives.” European Journal of\nCardio-Thoracic Surgery 53 (6): 1112–17. https://doi.org/10.1093/ejcts/ezy167.\n\n\nBen-Michael, Eli, Avi Feller, David A. Hirshberg, and José R.\nZubizarreta. 2021. “The Balancing Act in Causal\nInference.” arXiv:2110.14831 [Stat], October. https://arxiv.org/abs/2110.14831.\n\n\nBramante, Carolyn T., Steven G. Johnson, Victor Garcia, Michael D.\nEvans, Jeremy Harper, Kenneth J. Wilkins, Jared D. Huling, et al. 2022.\n“Diabetes Medications and Associations with Covid-19 Outcomes in\nthe N3C Database: A National Retrospective Cohort Study.” Edited\nby Surasak Saokaew. PLOS ONE 17 (11): e0271574. https://doi.org/10.1371/journal.pone.0271574.\n\n\nBrookhart, M. Alan, Til Stürmer, Robert J. Glynn, Jeremy Rassen, and\nSebastian Schneeweiss. 2010. “Confounding Control in Healthcare\nDatabase Research: Challenges and Potential Approaches.”\nMedical Care 48 (6): S114–20. https://doi.org/10.1097/MLR.0b013e3181dbebe3.\n\n\nCafri, Guy, and Peter C. Austin. 2023. “Variance Estimation of the\nRisk Difference When Using Propensity-Score Matching and\nWeighting with Time-to-Event Outcomes.”\nPharmaceutical Statistics, May, pst.2317. https://doi.org/10.1002/pst.2317.\n\n\nCaliendo, Marco, and Sabine Kopeinig. 2008. “Some Practical\nGuidance for the Implementation of Propensity Score Matching.”\nJournal of Economic Surveys 22 (1): 31–72. https://doi.org/10.1111/j.1467-6419.2007.00527.x.\n\n\nCarpenter, James, and John Bithell. 2000. “Bootstrap Confidence\nIntervals: When, Which, What? A Practical Guide for Medical\nStatisticians.” Statistics in Medicine 19 (9): 1141–64.\nhttps://doi.org/10.1002/(SICI)1097-0258(20000515)19:9&lt;1141::AID-SIM479&gt;3.0.CO;2-F.\n\n\nCham, Heining, and Stephen G. West. 2016. “Propensity Score\nAnalysis with Missing Data.” Psychological Methods 21\n(3): 427–45. https://doi.org/10.1037/met0000076.\n\n\nChan, Kwun Chuen Gary, Sheung Chi Phillip Yam, and Zheng Zhang. 2016.\n“Globally Efficient Non-Parametric Inference of Average Treatment\nEffects by Empirical Balancing Calibration Weighting.”\nJournal of the Royal Statistical Society: Series B (Statistical\nMethodology) 78 (3): 673–700. https://doi.org/10.1111/rssb.12129.\n\n\nChattopadhyay, Ambarish, Christopher H. Hase, and José R. Zubizarreta.\n2020. “Balancing Vs Modeling Approaches to Weighting in\nPractice.” Statistics in Medicine 39 (24): 3227–54. https://doi.org/10.1002/sim.8659.\n\n\nCohn, Eric R., and José R. Zubizarreta. 2022. “Profile Matching\nfor the Generalization and Personalization of Causal Inferences.”\nEpidemiology 33 (5): 678. https://doi.org/10.1097/EDE.0000000000001517.\n\n\nCole, Stephen R., and Constantine E. Frangakis. 2009. “The\nConsistency Statement in Causal Inference: A Definition or an\nAssumption?” Epidemiology 20 (1): 3–5. https://doi.org/10.1097/EDE.0b013e31818ef366.\n\n\nConnors, Alfred F, Neal V Dawson, Frank E Harrell, Douglas Wagner,\nNorman Desbiens, Lee Goldman, Albert W Wu, et al. 1996. “The\nEffectiveness of Right Heart Catheterization in the Initial Care of\nCritically III Patients.” JAMA: The Journal of the American\nMedical Association 276 (11): 889. https://doi.org/10.1001/jama.1996.03540110043030.\n\n\nD’Agostino McGowan, Lucy. 2022. “Sensitivity Analyses for\nUnmeasured Confounders.” Current Epidemiology Reports 9\n(4): 361–75. https://doi.org/10.1007/s40471-022-00308-6.\n\n\nDaniel, Rhian M. 2018. “Double Robustness.” In, 1–14.\nAmerican Cancer Society. https://doi.org/10.1002/9781118445112.stat08068.\n\n\nDesai, Rishi J., Kenneth J. Rothman, Brian .T Bateman, Sonia\nHernandez-Diaz, and Krista F. Huybrechts. 2017. “A\nPropensity-Score-Based Fine Stratification Approach for Confounding\nAdjustment When Exposure Is Infrequent:” Epidemiology 28\n(2): 249–57. https://doi.org/10.1097/EDE.0000000000000595.\n\n\nDiamond, Alexis, and Jasjeet S. Sekhon. 2013. “Genetic Matching\nfor Estimating Causal Effects: A General Multivariate Matching Method\nfor Achieving Balance in Observational Studies.” Review of\nEconomics and Statistics 95 (3): 932945. https://doi.org/10.1162/REST_a_00318.\n\n\nDong, Jing, Junni L Zhang, Shuxi Zeng, and Fan Li. 2020. “Subgroup\nBalancing Propensity Score.” Statistical Methods in Medical\nResearch 29 (3): 659–76. https://doi.org/10.1177/0962280219870836.\n\n\nEfron, B., and R. Tibshirani. 1986. “Bootstrap Methods for\nStandard Errors, Confidence Intervals, and Other Measures of Statistical\nAccuracy.” Statistical Science 1 (1): 54–75. https://www.jstor.org/stable/2245500.\n\n\nElwert, Felix, and Christopher Winship. 2014. “Endogenous\nSelection Bias: The Problem of Conditioning on a Collider\nVariable.” Annual Review of Sociology 40 (1): 31–53. https://doi.org/10.1146/annurev-soc-071913-043455.\n\n\nFogarty, Colin B., Mark E. Mikkelsen, David F. Gaieski, and Dylan S.\nSmall. 2016. “Discrete Optimization for Interpretable Study\nPopulations and Randomization Inference in an Observational Study of\nSevere Sepsis Mortality.” Journal of the American Statistical\nAssociation 111 (514): 447–58. https://doi.org/10.1080/01621459.2015.1112802.\n\n\nFortin, Stephen P., and Martijn Schuemie. 2022. “Indirect\nCovariate Balance and Residual Confounding: An Applied\nComparison of Propensity Score Matching and Cardinality\nMatching.” Pharmacoepidemiology and Drug Safety 31 (12):\n1242–52. https://doi.org/10.1002/pds.5510.\n\n\nGayat, Etienne, Matthieu Resche-Rigon, Jean-Yves Mary, and Raphaël\nPorcher. 2012. “Propensity Score Applied to Survival Data Analysis\nThrough Proportional Hazards Models: A Monte Carlo\nStudy.” Pharmaceutical Statistics 11 (3): 222–29. https://doi.org/10.1002/pst.537.\n\n\nGreen, Kerry M., and Elizabeth A. Stuart. 2014. “Examining\nModeration Analyses in Propensity Score Methods: Application to\nDepression and Substance Use.” Journal of Consulting and\nClinical Psychology, Advances in data analytic methods, 82 (5):\n773–83. https://doi.org/10.1037/a0036515.\n\n\nGreenland, Sander, Judea Pearl, and James M. Robins. 1999. “Causal\nDiagrams for Epidemiologic Research.” Epidemiology 10\n(1): 37–48. https://www.jstor.org/stable/3702180.\n\n\nGreifer, Noah. 2020. Cobalt: Covariate Balance Tables and\nPlots. https://CRAN.R-project.org/package=cobalt.\n\n\nGreifer, Noah, and Elizabeth A Stuart. 2021a. “Matching Methods\nfor Confounder Adjustment: An Addition to the\nEpidemiologist’s Toolbox.” Epidemiologic\nReviews, June, mxab003. https://doi.org/10.1093/epirev/mxab003.\n\n\nGreifer, Noah, and Elizabeth A. Stuart. 2021b. “Choosing the\nEstimand When Matching or Weighting in Observational Studies.”\narXiv:2106.10577 [Stat], June. https://arxiv.org/abs/2106.10577.\n\n\nGu, Xing Sam, and Paul R. Rosenbaum. 1993. “Comparison of\nMultivariate Matching Methods: Structures, Distances, and\nAlgorithms.” Journal of Computational and Graphical\nStatistics 2 (4): 405. https://doi.org/10.2307/1390693.\n\n\nHainmueller, J. 2012. “Entropy Balancing for Causal Effects: A\nMultivariate Reweighting Method to Produce Balanced Samples in\nObservational Studies.” Political Analysis 20 (1):\n25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nHan, Shasha, and Xiao-Hua Zhou. 2023. “Defining Estimands in\nClinical Trials: A Unified Procedure.” Statistics in\nMedicine, March, sim.9702. https://doi.org/10.1002/sim.9702.\n\n\nHaneuse, Sebastien, Tyler J. VanderWeele, and David Arterburn. 2019.\n“Using the e-Value to Assess the Potential Effect of Unmeasured\nConfounding in Observational Studies.” JAMA 321 (6):\n602–3. https://doi.org/10.1001/jama.2018.21554.\n\n\nHansen, Ben B, and Stephanie Olsen Klopfer. 2006. “Optimal Full\nMatching and Related Designs via Network Flows.” Journal of\nComputational and Graphical Statistics 15 (3): 609–27. https://doi.org/10.1198/106186006X137047.\n\n\nHarder, Valerie S., Elizabeth A. Stuart, and James C. Anthony. 2010.\n“Propensity Score Techniques and the Assessment of Measured\nCovariate Balance to Test Causal Associations in Psychological\nResearch.” Psychological Methods 15 (3): 234–49. https://doi.org/10.1037/a0019623.\n\n\nHernán, Miguel A. 2010. “The Hazards of Hazard Ratios.”\nEpidemiology (Cambridge, Mass.) 21 (1): 13–15. https://doi.org/10.1097/EDE.0b013e3181c1ea43.\n\n\nHernán, Miguel A., and James M. Robins. 2006a. “Instruments for\nCausal Inference: An Epidemiologist’s Dream?”\nEpidemiology 17 (4): 360–72. https://doi.org/10.1097/01.ede.0000222409.00878.37.\n\n\nHernán, Miguel A, and James M Robins. 2020. Causal Inference: What\nIf. Boca Raton: Chapman & Hall/CRC. https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf.\n\n\nHernán, Miguel A, and James M. Robins. 2006b. “Estimating Causal\nEffects from Epidemiological Data.” Journal of Epidemiology\nand Community Health (1979-) 60 (7): 578–86. http://www.jstor.org/stable/40795098.\n\n\nHernán, Miguel A, and S L Taubman. 2008. “Does Obesity Shorten\nLife? The Importance of Well-Defined Interventions to Answer Causal\nQuestions.” International Journal of Obesity 32 (S3):\nS8–14. https://doi.org/10.1038/ijo.2008.82.\n\n\nHill, Jennifer, and Jerome P. Reiter. 2006. “Interval Estimation\nfor Treatment Effects Using Propensity Score Matching.”\nStatistics in Medicine 25 (13): 2230–56. https://doi.org/10.1002/sim.2277.\n\n\nHill, Jennifer, Christopher Weiss, and Fuhua Zhai. 2011.\n“Challenges With Propensity Score Strategies in a High-Dimensional\nSetting and a Potential Alternative.” Multivariate Behavioral\nResearch 46 (3): 477–513. https://doi.org/10.1080/00273171.2011.570161.\n\n\nHirano, Keisuke, and Guido W. Imbens. 2005. “The Propensity Score\nwith Continuous Treatments.” In, edited by Andrew Gelman and\nXiao-Li Meng, 73–84. Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/0470090456.ch7.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007.\n“Matching as Nonparametric Preprocessing for Reducing Model\nDependence in Parametric Causal Inference.” Political\nAnalysis 15 (3): 199–236. https://doi.org/10.1093/pan/mpl013.\n\n\nHong, Guanglei. 2010. “Marginal Mean Weighting Through\nStratification: Adjustment for Selection Bias in Multilevel\nData.” Journal of Educational and Behavioral Statistics\n35 (5): 499–531. https://doi.org/10.3102/1076998609359785.\n\n\nHuber, Martin. 2015. “Causal Pitfalls in the\nDecomposition of Wage Gaps.”\nJournal of Business & Economic Statistics 33 (2): 179–91.\nhttps://doi.org/10.1080/07350015.2014.937437.\n\n\nHuling, Jared D., Noah Greifer, and Guanhua Chen. 2023.\n“Independence Weights for Causal Inference with Continuous\nTreatments.” Journal of the American Statistical\nAssociation 0 (ja): 1–25. https://doi.org/10.1080/01621459.2023.2213485.\n\n\nHuling, Jared D., and Simon Mak. n.d. “Energy Balancing of\nCovariate Distributions.” https://doi.org/10.48550/arXiv.2004.13962.\n\n\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2011.\n“Multivariate Matching Methods That Are Monotonic Imbalance\nBounding.” Journal of the American Statistical\nAssociation 106 (493): 345–61. https://doi.org/10.1198/jasa.2011.tm09599.\n\n\n———. 2012. “Causal Inference Without Balance Checking: Coarsened\nExact Matching.” Political Analysis 20 (1): 1–24. https://doi.org/10.1093/pan/mpr013.\n\n\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008.\n“Misunderstandings Between Experimentalists and Observationalists\nabout Causal Inference.” Journal of the Royal Statistical\nSociety. Series A (Statistics in Society) 171 (2): 481–502. https://doi.org/10.1111/j.1467-985X.2007.00527.x.\n\n\nImai, Kosuke, and Marc Ratkovic. 2014. “Covariate Balancing\nPropensity Score.” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 76 (1): 243263. https://doi.org/10.1111/rssb.12027.\n\n\nImai, Kosuke, and David A. Van Dyk. 2004. “Causal Inference with\nGeneral Treatment Regimes: Generalizing the Propensity Score.”\nJournal of the American Statistical Association 99 (467):\n854–66. https://www.jstor.org/stable/27590455.\n\n\nImbens, Guido W. 2000. “The Role of the Propensity Score in\nEstimating Dose-Response Functions.” Biometrika 87 (3):\n706–10. https://www.jstor.org/stable/2673642.\n\n\nIoannidis, John P. A., Yuan Jin Tan, and Manuel R. Blum. 2019.\n“Limitations and Misinterpretations of e-Values for Sensitivity\nAnalyses of Observational Studies.” Annals of Internal\nMedicine 170 (2): 108–11. https://doi.org/10.7326/M18-2159.\n\n\nKahan, Brennan C, Suzie Cro, Fan Li, and Michael O Harhay. 2023.\n“Eliminating Ambiguous Treatment Effects Using Estimands.”\nAmerican Journal of Epidemiology, February, kwad036. https://doi.org/10.1093/aje/kwad036.\n\n\nKang, Joseph D. Y., and Joseph L. Schafer. 2007. “Demystifying\nDouble Robustness: A Comparison of Alternative Strategies for Estimating\na Population Mean from Incomplete Data.” Statistical\nScience 22 (4): 523–39. https://doi.org/10.1214/07-STS227.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis,\nMay, 1–20. https://doi.org/10.1017/pan.2019.11.\n\n\nKing, Gary, and Langche Zeng. 2006. “The Dangers of Extreme\nCounterfactuals.” Political Analysis 14 (2): 131–59. https://doi.org/10.1093/pan/mpj004.\n\n\nKush, Joseph M., Elise T. Pas, Rashelle J. Musci, and Catherine P.\nBradshaw. 2022. “Covariate Balance for Observational Effectiveness\nStudies: A Comparison of Matching and Weighting.” Journal of\nResearch on Educational Effectiveness 0 (0): 1–24. https://doi.org/10.1080/19345747.2022.2110545.\n\n\nLeyrat, Clémence, Shaun R Seaman, Ian R White, Ian Douglas, Liam Smeeth,\nJoseph Kim, Matthieu Resche-Rigon, James R Carpenter, and Elizabeth J\nWilliamson. 2019. “Propensity Score Analysis with Partially\nObserved Covariates: How Should Multiple Imputation Be Used?”\nStatistical Methods in Medical Research 28 (1): 3–19. https://doi.org/10.1177/0962280217713032.\n\n\nLi, Fan, and Fan Li. 2019. “Propensity Score Weighting for Causal\nInference with Multiple Treatments.” The Annals of Applied\nStatistics 13 (4): 2389–2415. https://doi.org/10.1214/19-AOAS1282.\n\n\nLi, Liang, and Tom Greene. 2013. “A Weighting Analogue to Pair\nMatching in Propensity Score Analysis.” The International\nJournal of Biostatistics 9 (2). https://doi.org/10.1515/ijb-2012-0030.\n\n\nLi, Yan, and Liang Li. 2021. “Propensity Score Analysis Methods\nwith Balancing Constraints: A Monte Carlo Study.” Statistical\nMethods in Medical Research 30 (4): 1119–42. https://doi.org/10.1177/0962280220983512.\n\n\nLiang, Kung-Yee, and Scott L. Zeger. 1986. “Longitudinal Data\nAnalysis Using Generalized Linear Models.” Biometrika 73\n(1): 13–22. https://doi.org/10.1093/biomet/73.1.13.\n\n\nLopez, Michael J., and Roee Gutman. 2017. “Estimation of Causal\nEffects with Multiple Treatments: A Review and New Ideas.”\nStatistical Science 32 (3): 432–54. https://doi.org/10.1214/17-STS612.\n\n\nMacKinnon, James G, and Halbert White. 1985. “Some\nHeteroskedasticity-Consistent Covariance Matrix Estimators with Improved\nFinite Sample Properties.” Journal of Econometrics 29\n(3): 305–25. https://doi.org/10.1016/0304-4076(85)90158-7.\n\n\nMao, Huzhang, Liang Li, and Tom Greene. 2018. “Propensity Score\nWeighting Analysis and Treatment Effect Discovery.”\nStatistical Methods in Medical Research, June, 096228021878117.\nhttps://doi.org/10.1177/0962280218781171.\n\n\nMatthay, Ellicott C., Erin Hagan, Laura M. Gottlieb, May Lynn Tan, David\nVlahov, Nancy E. Adler, and M. Maria Glymour. 2020. “Alternative\nCausal Inference Methods in Population Health Research: Evaluating\nTradeoffs and Triangulating Evidence.” SSM - Population\nHealth 10 (April): 100526. https://doi.org/10.1016/j.ssmph.2019.100526.\n\n\nMcCaffrey, Daniel F., Beth Ann Griffin, Daniel Almirall, Mary Ellen\nSlaughter, Rajeev Ramchand, and Lane F. Burgette. 2013. “A\nTutorial on Propensity Score Estimation for Multiple Treatments Using\nGeneralized Boosted Models.” Statistics in Medicine 32\n(19): 3388–3414. https://doi.org/10.1002/sim.5753.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004.\n“Propensity Score Estimation With Boosted Regression for\nEvaluating Causal Effects in Observational Studies.”\nPsychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nMing, Kewei, and Paul R. Rosenbaum. 2000. “Substantial Gains in\nBias Reduction from Matching with a Variable Number of Controls.”\nBiometrics 56 (1): 118–24. https://doi.org/10.1111/j.0006-341X.2000.00118.x.\n\n\nNiknam, Bijan A., and Jose R. Zubizarreta. 2022. “Using\nCardinality Matching to Design Balanced and\nRepresentative Samples for Observational\nStudies.” JAMA 327 (2): 173–74. https://doi.org/10.1001/jama.2021.20555.\n\n\nPak, Kyongsun, Hajime Uno, Dae Hyun Kim, Lu Tian, Robert C. Kane,\nMasahiro Takeuchi, Haoda Fu, Brian Claggett, and Lee-Jen Wei. 2017.\n“Interpretability of Cancer Clinical Trial Results Using\nRestricted Mean Survival Time as an Alternative to the Hazard\nRatio.” JAMA Oncology 3 (12): 1692. https://doi.org/10.1001/jamaoncol.2017.2797.\n\n\nPishgar, Farhad, Noah Greifer, Clémence Leyrat, and Elizabeth Stuart.\n2021. “MatchThem:: Matching and Weighting After Multiple\nImputation.” The R Journal 13 (2): 292305. https://doi.org/10.32614/RJ-2021-073.\n\n\nReifeis, Sarah A., and Michael G. Hudgens. 2020. “On Variance of\nthe Treatment Effect in the Treated Using Inverse Probability\nWeighting.” arXiv:2011.11874 [Stat], November. http://arxiv.org/abs/2011.11874.\n\n\nRidgeway, Greg. 2006. “Assessing the Effect of Race Bias in\nPost-Traffic Stop Outcomes Using Propensity Scores.” Journal\nof Quantitative Criminology 22 (1): 1–29. https://doi.org/10.1007/s10940-005-9000-9.\n\n\nRipollone, John E., Krista F. Huybrechts, Kenneth J. Rothman, Ryan E.\nFerguson, and Jessica M. Franklin. 2018. “Implications of the\nPropensity Score Matching Paradox in Pharmacoepidemiology.”\nAmerican Journal of Epidemiology 187 (9): 1951–61. https://doi.org/10.1093/aje/kwy078.\n\n\nRobins, James M., Miguel Ángel Hernán, and Babette Brumback. 2000.\n“Marginal Structural Models and Causal Inference in\nEpidemiology.” Epidemiology 11 (5): 550–60. https://doi.org/10.1097/00001648-200009000-00011.\n\n\nRosenbaum, Paul R. 2007. “Interference Between Units in Randomized\nExperiments.” Journal of the American Statistical\nAssociation 102 (477): 191–200. https://doi.org/10.1198/016214506000001112.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The Central Role\nof the Propensity Score in Observational Studies for Causal\nEffects.” Biometrika 70 (1): 41–55. https://doi.org/10.1093/biomet/70.1.41.\n\n\n———. 1984. “Reducing Bias in Observational Studies Using\nSubclassification on the Propensity Score.” Journal of the\nAmerican Statistical Association 79 (387): 516–24. https://doi.org/10.2307/2288398.\n\n\n———. 1985. “The Bias Due to Incomplete Matching.”\nBiometrics 41 (1): 103–16. https://doi.org/10.2307/2530647.\n\n\nRubin, Donald B. 1973. “Matching to Remove Bias in Observational\nStudies.” Biometrics 29 (1): 159–83. https://doi.org/10.2307/2529684.\n\n\n———. 2004. Multiple Imputation for Nonresponse in Surveys.\nWiley Classics Library. Hoboken, N.J: Wiley-Interscience.\n\n\nSävje, Fredrik, Michael J. Higgins, and Jasjeet S. Sekhon. 2021.\n“Generalized Full Matching.” Political Analysis 29\n(4): 423–47. https://doi.org/10.1017/pan.2020.32.\n\n\nShadish, William R., and Peter M. Steiner. 2010. “A Primer on\nPropensity Score Analysis.” Newborn and Infant Nursing\nReviews, Quantitative research methodology, 10 (1): 19–26. https://doi.org/10.1053/j.nainr.2009.12.010.\n\n\nSharma, Mayur, Truong H. Do, Elise F. Palzer, Jared D. Huling, and Clark\nC. Chen. 2023. “Comparable Safety Profile Between Neuro-Oncology\nProcedures Involving Stereotactic Needle Biopsy (SNB) Followed by Laser\nInterstitial Thermal Therapy (LITT) and LITT Alone Procedures.”\nJournal of Neuro-Oncology 162 (1): 147–56. https://doi.org/10.1007/s11060-023-04275-w.\n\n\nSnowden, Jonathan M., Sherri Rose, and Kathleen M. Mortimer. 2011.\n“Implementation of G-Computation on a Simulated Data Set:\nDemonstration of a Causal Inference Technique.” American\nJournal of Epidemiology 173 (7): 731–38. https://doi.org/10.1093/aje/kwq472.\n\n\nStensrud, Mats J., and Miguel A. Hernán. 2020. “Why Test for\nProportional Hazards?” JAMA 323 (14): 1401–2. https://doi.org/10.1001/jama.2020.1267.\n\n\nStrasser, Zachary H., Noah Greifer, Aboozar Hadavand, Shawn N. Murphy,\nand Hossein Estiri. 2022. “Estimates of SARS-CoV-2 Omicron BA.2\nSubvariant Severity in New England.” JAMA Network Open 5\n(10): e2238354. https://doi.org/10.1001/jamanetworkopen.2022.38354.\n\n\nStuart, Elizabeth A. 2010. “Matching Methods for Causal Inference:\nA Review and a Look Forward.” Statistical Science 25\n(1): 1–21. https://doi.org/10.1214/09-STS313.\n\n\nStuart, Elizabeth A., and Kerry M. Green. 2008. “Using Full\nMatching to Estimate Causal Effects in Nonexperimental Studies:\nExamining the Relationship Between Adolescent Marijuana Use and Adult\nOutcomes.” Developmental Psychology, New methods for new\nquestions in developmental psychology, 44 (2): 395–406. https://doi.org/10.1037/0012-1649.44.2.395.\n\n\nStuart, Elizabeth A., Brian K. Lee, and Finbarr P. Leacy. 2013.\n“Prognostic Score-Based Balance Measures Can Be a Useful\nDiagnostic for Propensity Score Methods in Comparative Effectiveness\nResearch.” Journal of Clinical Epidemiology 66 (8): S84.\nhttps://doi.org/10.1016/j.jclinepi.2013.01.013.\n\n\nTchetgen, Eric J Tchetgen, and Tyler J VanderWeele. 2012. “On\nCausal Inference in the Presence of Interference.”\nStatistical Methods in Medical Research 21 (1): 55–75. https://doi.org/10.1177/0962280210386779.\n\n\nThoemmes, Felix J., and Anthony D. Ong. 2016. “A Primer on Inverse\nProbability of Treatment Weighting and Marginal Structural\nModels.” Emerging Adulthood 4 (1): 40–59. https://doi.org/10.1177/2167696815621645.\n\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between\nInteraction and Effect Modification.” Epidemiology 20\n(6): 863–71. https://www.jstor.org/stable/25662776.\n\n\n———. 2019. “Principles of Confounder Selection.”\nEuropean Journal of Epidemiology 34 (3): 211–19. https://doi.org/10.1007/s10654-019-00494-6.\n\n\nVanderWeele, Tyler J., and Peng Ding. 2017. “Sensitivity Analysis\nin Observational Research: Introducing the E-Value.” Annals\nof Internal Medicine 167 (4): 268. https://doi.org/10.7326/M16-2607.\n\n\nVanderWeele, Tyler J., Maya B. Mathur, and Peng Ding. 2019.\n“Correcting Misinterpretations of the e-Value.” Annals\nof Internal Medicine 170 (2): 131–32. https://doi.org/10.7326/M18-3112.\n\n\nVansteelandt, Stijn, and Niels Keiding. 2011. “Invited Commentary:\nG-Computationlost in Translation?” American\nJournal of Epidemiology 173 (7): 739–42. https://doi.org/10.1093/aje/kwq474.\n\n\nVisconti, Giancarlo, and José R. Zubizarreta. 2018. “Handling\nLimited Overlap in Observational Studies with Cardinality\nMatching.” Observational Studies 4 (1): 217–49. https://doi.org/10.1353/obs.2018.0012.\n\n\nWan, Fei. 2019. “Matched or Unmatched Analyses with\nPropensity-Scorematched Data?” Statistics in Medicine 38\n(2): 289–300. https://doi.org/10.1002/sim.7976.\n\n\nWestreich, Daniel, and Stephen R. Cole. 2010. “Invited Commentary:\nPositivity in Practice.” American Journal of\nEpidemiology 171 (6): 674–77. https://doi.org/10.1093/aje/kwp436.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The\nTable 2 Fallacy: Presenting and\nInterpreting Confounder and Modifier\nCoefficients.” American Journal of Epidemiology\n177 (4): 292–98. https://doi.org/10.1093/aje/kws412.\n\n\nWu, Xiao, Fabrizia Mealli, Marianthi-Anna Kioumourtzoglou, Francesca\nDominici, and Danielle Braun. 2022. “Matching on Generalized\nPropensity Scores with Continuous Exposures.” Journal of the\nAmerican Statistical Association 0 (0): 1–29. https://doi.org/10.1080/01621459.2022.2144737.\n\n\nZakrison, T. L., Peter C. Austin, and V. A. McCredie. 2018. “A\nSystematic Review of Propensity Score Methods in the Acute Care Surgery\nLiterature: Avoiding the Pitfalls and Proposing a Set of Reporting\nGuidelines.” European Journal of Trauma and Emergency\nSurgery 44 (3): 385–95. https://doi.org/10.1007/s00068-017-0786-6.\n\n\nZhao, Qingyuan, and Daniel Percival. 2017. “Entropy Balancing Is\nDoubly Robust.” Journal of Causal Inference 5 (1). https://doi.org/10.1515/jci-2016-0010.\n\n\nZhu, Yeying, Donna L. Coffman, and Debashis Ghosh. 2015. “A\nBoosting Algorithm for Estimating Generalized Propensity Scores with\nContinuous Treatments.” Journal of Causal Inference 3\n(1). https://doi.org/10.1515/jci-2014-0022.\n\n\nZubizarreta, José R. 2015. “Stable Weights That Balance Covariates\nfor Estimation with Incomplete Outcome Data.” Journal of the\nAmerican Statistical Association 110 (511): 910–22. https://doi.org/10.1080/01621459.2015.1023805.\n\n\nZubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014.\n“Matching for Balance, Pairing for Heterogeneity in an\nObservational Study of the Effectiveness of for-Profit and\nNot-for-Profit High Schools in Chile.” The\nAnnals of Applied Statistics 8 (1): 204–31. https://doi.org/10.1214/13-AOAS713."
  }
]